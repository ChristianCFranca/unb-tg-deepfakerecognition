{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este notebook tem como função criar uma pasta chamada \"Kaggle Faces Dataset\" onde os rostos recortados dos vídeos serão guardados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, carregamos as dependências necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define-se o device onde será rodada a detecção de rostos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0, NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# Definimos um device onde os tensores estarão sendo processados\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}, {}'.format(device, torch.cuda.get_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1° passo: Criar uma classe que cuida de todas as leituras dos vídeos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria-se uma classe que cuida da leitura de todos os vídeos do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuida de lidar com o acesso aos vídeos e devolver os paths / labels corretamente\n",
    "class Videos():\n",
    "    def __init__(self, root=\"./Kaggle Dataset\"):\n",
    "        # Guarda o folder_path\n",
    "        self.root = Path(root)\n",
    "        \n",
    "        folders = list(self.root.glob(\"*\"))\n",
    "        order = lambda x: int(x.stem.split(\"_\")[-1])\n",
    "        self.folders = sorted(folders, key=order)\n",
    "        \n",
    "    def getRandomVideo(self):\n",
    "        # Lê o arquivo JSON que contém as informações dos deepfakes naquela pasta\n",
    "        \n",
    "        folder_path = random.choice(self.folders)\n",
    "        \n",
    "        metadata = pd.read_json(folder_path/'metadata.json').transpose() # Troca colunas e linhas para ficarEM no formato correto\n",
    "        \n",
    "        video_path = random.choice(list(folder_path)) # Devolve um WindowsPath\n",
    "        \n",
    "        video_name = video_path.name # videoaleatorio.mp4\n",
    "        \n",
    "        label = metadata.loc[video_name].label\n",
    "        \n",
    "        return video_path, video_name, label\n",
    "    \n",
    "    def getRandomLabeledVideo(self, label):\n",
    "        if label not in [\"FAKE\", \"REAL\"]:\n",
    "            print(\"Label deve ser FAKE ou REAL\")\n",
    "            return\n",
    "        \n",
    "        folder_path = random.choice(self.folders) # \"./Kaggle Dataset/some_folder\"\n",
    "        \n",
    "        metadata = pd.read_json(folder_path/'metadata.json').transpose() # Troca colunas e linhas para ficarEM no formato correto\n",
    "        \n",
    "        fake_videos = metadata[metadata.label == label].index # Filtra os videos que são FAKE\n",
    "\n",
    "        video_path = folder_path/random.choice(fake_videos)\n",
    "        \n",
    "        video_name = video_path.name # videoaleatorio.mp4\n",
    "        \n",
    "        label = label\n",
    "        \n",
    "        return video_path, video_name, label\n",
    "        \n",
    "    def getEquivalentRealVideo(self, video_path):\n",
    "        \n",
    "        if not isinstance(video_path, pathlib.WindowsPath):\n",
    "            video_path = Path(video_path)\n",
    "            if not video_path.exists():\n",
    "                print(\"Video path não existe.\")\n",
    "                return\n",
    "            \n",
    "        folder_path = video_path.parent\n",
    "        \n",
    "        metadata = pd.read_json(folder_path/'metadata.json').transpose() # Troca colunas e linhas para ficarEM no formato correto\n",
    "        \n",
    "        try: \n",
    "            video = metadata.loc[video_path.name]\n",
    "        except:\n",
    "            print(\"O vídeo indicado não existe no metadata.\")\n",
    "            return\n",
    "        \n",
    "        if video.label == \"REAL\":\n",
    "            print(\"O vídeo em questão já é um vídeo real.\")\n",
    "            return\n",
    "        \n",
    "        real_video_name = video.original\n",
    "        real_video_path = folder_path.joinpath(real_video_name)\n",
    "        label = \"REAL\"\n",
    "        \n",
    "        return real_video_path, real_video_name, label\n",
    "    \n",
    "    def getAllVideosPath(self):\n",
    "        for video_name, columns in self.metadata.iterrows():\n",
    "            yield self.folder_path + '/' + video_name, video_name, columns[0] # Label\n",
    "            \n",
    "    def generateMetadataTestSplit(self, proportion=0.2): # 20% para o set de treinamento\n",
    "        print(\"Ajuste de metadata.json em andamento...\")\n",
    "        np.random.seed(42)\n",
    "        for folder in self.folders:\n",
    "            metadata_path = folder/\"metadata.json\"\n",
    "            metadata = pd.read_json(metadata_path).transpose()\n",
    "            if \"test\" not in metadata.values:\n",
    "                counts = metadata.label.value_counts()\n",
    "\n",
    "                reals_quantity_to_test = int(proportion*counts[\"REAL\"])\n",
    "                fakes_quantity_to_test = int(proportion*counts[\"FAKE\"])\n",
    "\n",
    "                real_videos = metadata[metadata.label == 'REAL'].index\n",
    "                fake_videos = metadata[metadata.label == 'FAKE'].index\n",
    "\n",
    "                reals_to_test = np.random.choice(real_videos, size=reals_quantity_to_test)\n",
    "                fakes_to_test = np.random.choice(fake_videos, size=fakes_quantity_to_test)\n",
    "\n",
    "                metadata.loc[reals_to_test, 'split'] = \"test\"\n",
    "                metadata.loc[fakes_to_test, 'split'] = \"test\"\n",
    "            else:\n",
    "                print(\"Pass...\")\n",
    "            metadata.transpose().to_json(metadata_path)\n",
    "        print(\"Finalizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função **showVideo()**: Mostra o vídeo para checagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVideo(video_path, resize_factor=0.6):\n",
    "    \n",
    "    # Captura o vídeo no path\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read() # Lê o próximo frame\n",
    "        if ret: # Sucesso na leitura\n",
    "            frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "            cv2.imshow(f\"{video_path.parts[1]}/{video_path.parts[2]}\", frame)\n",
    "            \n",
    "            # Apertar a tecla 'q' para sair do vídeo.\n",
    "            key = cv2.waitKey(25)\n",
    "            if key == 113:\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVideoWithDetection(video_path, label=None, padding=0, size=-1, separate_face_box=False, resize_factor=0.6):\n",
    "    \n",
    "    # Captura o vídeo no path\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    # Configura a cor a ser colocada na LABEL\n",
    "    if label == 'REAL':\n",
    "        color = (0, 255, 0) # Verde\n",
    "    elif label == \"FAKE\":\n",
    "        color = (0, 0, 255) # Vermelho  \n",
    "    else:\n",
    "        color = (0, 255, 255)\n",
    "    \n",
    "    face = None\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read() # Lê o próximo frame\n",
    "        if ret: # Sucesso na leitura\n",
    "            boxes, _ = mtcnn.detect(Image.fromarray(frame)) # Detecta as imagens. O método detect só aceita numpy arrays\n",
    "            if boxes is not None: # Só entra se rostos forem detectados\n",
    "                for box in boxes: # Para cada uma das bouding boxes encontradas em um único frame (a princípio só deve ter uma)\n",
    "                    box = [int(b) for b in box]\n",
    "                    if separate_face_box:\n",
    "                        face = frame[int(box[1] - padding):int(box[3] + padding), int(box[0] - padding):int(box[2] + padding)].copy()\n",
    "                        if face is not None:\n",
    "                            if size > 0:\n",
    "                                face = cv2.resize(face, (size, size))\n",
    "                            cv2.imshow('face', face)\n",
    "                    cv2.putText(img=frame, text=label, org=(box[0], box[1]), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=color, thickness=2)\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color=[0, 255, 0], thickness=5)\n",
    "\n",
    "            frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "            cv2.imshow(video_path.name, frame)\n",
    "            \n",
    "            # Apertar a tecla 'q' para sair do vídeo.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2° passo: Observar alguns vídeos e ajustar o threshold do MTCNN para a detecção de rostos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MTCNN** - *Multi-task Cascaded Convolutional Network*\n",
    "\n",
    "Ajustamos os thresholds e utilizamos a função de mostrar os vídeos, verificando visualmente se ele se comporta bem na maioria dos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Margin não faz diferença se o método .detect() for utilizado\n",
    "IMAGE_SIZE = 224\n",
    "MARGIN = 0\n",
    "MIN_FACE_SIZE = 90\n",
    "THRESHOLDS = [0.68, 0.75, 0.80]\n",
    "POST_PROCESS = False\n",
    "SELECT_LARGEST = True\n",
    "KEEP_ALL = False\n",
    "DEVICE = device\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "mtcnn = MTCNN(image_size=IMAGE_SIZE,\n",
    "              margin=MARGIN, \n",
    "              min_face_size=MIN_FACE_SIZE, \n",
    "              thresholds=THRESHOLDS,\n",
    "              post_process=POST_PROCESS,\n",
    "              select_largest=SELECT_LARGEST, \n",
    "              keep_all=KEEP_ALL, \n",
    "              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objeto Videos\n",
    "\n",
    "Definimos nosso objeto vídeos com o diretório onde contém as pastas do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instaciamos uma objeto da classe Videos\n",
    "videos = Videos(root=\"./Kaggle Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vídeo Real Aleatório\n",
    "\n",
    "Para cancelar a visualização do vídeo em tempo real, apertar a tecla `q` do teclado.Para cancelar a visualização do vídeo em tempo real, apertar a tecla `q` do teclado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path, video_name, label = videos.getRandomLabeledVideo(label=\"REAL\")\n",
    "\n",
    "# Podemos passar um padding para verificar a quantidade desejada de recorte ao redo do rosto detectado\n",
    "showVideo(video_path)\n",
    "showVideoWithDetection(video_path, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vídeo Falso Aleatório\n",
    "\n",
    "Para cancelar a visualização do vídeo em tempo real, apertar a tecla `q` do teclado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path, video_name, label = videos.getRandomLabeledVideo(label=\"FAKE\") # Recupera o caminho de um vídeo aleatório na pasta que esteja com a label 'FAKE'\n",
    "\n",
    "showVideo(video_path)\n",
    "showVideoWithDetection(video_path, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vídeo Falso Aleatório e sua versão REAL\n",
    "\n",
    "Para cancelar a visualização do vídeo em tempo real, apertar a tecla `q` do teclado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path, video_name, label = videos.getRandomLabeledVideo(label=\"FAKE\") # Recupera o caminho de um vídeo aleatório na pasta que esteja com a label 'FAKE'\n",
    "video_path_real, video_name_real, label_real = videos.getEquivalentRealVideo(video_path)\n",
    "\n",
    "showVideo(video_path)\n",
    "showVideo(video_path_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo estando ok, agora podemos separar todos os vídeos em um conjunto de treinamento para realizar a validação cruzada (80%) e um conjunto de vídeos finais para teste (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos.generateMetadataTestSplit(proportion=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3° Passo: Criar uma classe que recebe um vídeo, detecta todos os rostos, rescorta eles e os salva no diretório em questão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classe **CropSaver()**: Recorta todos os rostos de um vídeo dada uma taxa de checagem por frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropSaver(Videos):\n",
    "    \n",
    "    def __init__(self, root=\"./Kaggle Dataset\", device=\"cuda\"):\n",
    "        super().__init__(root)\n",
    "        # Instaciamos a MTCNN de forma interna\n",
    "        IMAGE_SIZE = 224\n",
    "        MARGIN = 0\n",
    "        MIN_FACE_SIZE = 90\n",
    "        THRESHOLDS = [0.68, 0.75, 0.80]\n",
    "        POST_PROCESS = False\n",
    "        SELECT_LARGEST = True\n",
    "        KEEP_ALL = False\n",
    "        DEVICE = device\n",
    "\n",
    "        # ----------------------------------\n",
    "\n",
    "        self.mtcnn = MTCNN(image_size=IMAGE_SIZE,\n",
    "                      margin=MARGIN, \n",
    "                      min_face_size=MIN_FACE_SIZE, \n",
    "                      thresholds=THRESHOLDS,\n",
    "                      post_process=POST_PROCESS,\n",
    "                      select_largest=SELECT_LARGEST, \n",
    "                      keep_all=KEEP_ALL, \n",
    "                      device=DEVICE).eval()\n",
    "\n",
    "    def callCropFaces(self, path_to_save=\"./Kaggle Faces Dataset\", call=\"test-video\", \n",
    "                      check_every_frame=30, batch_size=40, padding=0, size=-1):\n",
    "        path_to_save = Path(path_to_save)\n",
    "        if call == \"test-video\":\n",
    "            video_path = list(self.folders[0].glob(\"*.mp4\"))[0]\n",
    "            metadata = pd.read_json(self.folders[0]/\"metadata.json\").transpose()\n",
    "            video_name = video_path.name\n",
    "            label = metadata.loc[video_name].label\n",
    "            split = metadata.loc[video_name].split\n",
    "            self.saveCropFaces(video_path, path_to_save, split, label, batch_size=batch_size,\n",
    "                               padding=padding, size=size, check_every_frame=check_every_frame)\n",
    "            print(\"All Good! Go check it!\")\n",
    "            \n",
    "        elif call == \"test-folder\":\n",
    "            videos_path = list(self.folders[0].glob(\"*.mp4\"))\n",
    "            metadata = pd.read_json(self.folders[0]/\"metadata.json\").transpose()\n",
    "            print(\"-\"*20 + f\"Beggining folder {self.folders[0].parts[-1]}...\" + \"-\"*20 )\n",
    "            for video_path in tqdm(videos_path):\n",
    "                video_name = video_path.name\n",
    "                label = metadata.loc[video_name].label\n",
    "                split = metadata.loc[video_name].split\n",
    "                self.saveCropFaces(video_path, path_to_save, split, label, batch_size=batch_size,\n",
    "                                   padding=padding, size=size, check_every_frame=check_every_frame)\n",
    "                \n",
    "        elif call == \"all\":\n",
    "            for folder in tqdm(self.folders):\n",
    "                videos_path = list(folder.glob(\"*.mp4\"))\n",
    "                metadata = pd.read_json(folder/\"metadata.json\").transpose()\n",
    "                print(\"-\"*20 + f\"Beggining folder {folder.parts[-1]}...\" + \"-\"*20 )\n",
    "                for video_path in tqdm(videos_path):\n",
    "                    video_name = video_path.name\n",
    "                    label = metadata.loc[video_name].label\n",
    "                    split = metadata.loc[video_name].split\n",
    "                    self.saveCropFaces(video_path, path_to_save, split, label, batch_size=batch_size,\n",
    "                                       padding=padding, size=size, check_every_frame=check_every_frame)\n",
    "\n",
    "    def saveCropFaces(self, video_path, path_to_save, split, label, batch_size=20, padding=0, size=-1, check_every_frame=30):\n",
    "        \n",
    "        # Instancia um VideoCapture do arquivo presente em video_path (no caso, o vídeo)\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        \n",
    "        # Pega, em inteiros, a quantidade de frames do vídeo\n",
    "        v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Cria um path para salvar os rostos recortados do vídeo\n",
    "        class_path = path_to_save.joinpath(split, label) # Path(./Kaggle Faces Dataset/train/FAKE ou REAL)\n",
    "        \n",
    "        if not class_path.exists():\n",
    "            print(f\"The folder {class_path} não é alcançável.\")\n",
    "            return\n",
    "        \n",
    "        # Inicializa frames como uma lista vazia\n",
    "        frames = []\n",
    "        face_count = 0\n",
    "        \n",
    "        # Entra num loop que percorre o vídeo até ele acabar\n",
    "        for _ in range(1, v_len + 1):\n",
    "            # Realiza um grab() no próximo frame, mas não o decodifica. Isso ajuda a agilizar o processo se não for necessário\n",
    "            # recuperar todos os frames a todo o momento.\n",
    "            success = cap.grab()\n",
    "            # Só recorta o rosto se o frame atual for mod check_every_frame, ou seja, ele só decodifica de check_every_frame em check_every_frame frames.\n",
    "            if not success:\n",
    "                continue\n",
    "            if _ == 1 or _ % check_every_frame == 0:\n",
    "                if class_path.joinpath(f\"{video_path.parts[-2]} {video_path.parts[-1]} {face_count+1} .jpg\").exists():\n",
    "                    face_count += 1\n",
    "                    continue\n",
    "                success, frame = cap.retrieve()\n",
    "            else:\n",
    "                continue\n",
    "            if not success:\n",
    "                continue\n",
    "            # Realiza um append do frame atual na lista frames (ele é capturado no formato BGR porém a MTCNN espera no formato RGB)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frames.append(frame)\n",
    "            \n",
    "            # Se o tamanho dos frames alcançou o batch_size OU o meu iterador de frames chegou no máixmo juntamento com meu tamanho de frames dentro da lista sendo maior que zero, continua pra capturar o rosto\n",
    "            if len(frames) >= batch_size or (_ == v_len and len(frames) > 0):\n",
    "                # Utiliza o MTCNN para detectar todas as bounding boxes de todos os rostos\n",
    "                boxes, probs = self.mtcnn.detect(frames)\n",
    "                # Verifica se não foi obtida nenhuma bounding box em todo o batch\n",
    "                if not all(x is None for x in boxes):\n",
    "                    # Acessa cada um dos frames no batch\n",
    "                    for i, boxes_f in enumerate(boxes):\n",
    "                        # Verifica houve None para o frame atual\n",
    "                        if boxes_f is not None:\n",
    "                            # Acessa cada uma das bounding boxes dentro de um único frame (pode haver vários rostos)\n",
    "                            for bbox in boxes_f:\n",
    "                                # Obtém o rosto\n",
    "                                face = frames[i].crop(box=(bbox[0]-padding, \n",
    "                                                           bbox[1]-padding, \n",
    "                                                           bbox[2]+padding, \n",
    "                                                           bbox[3]+padding))\n",
    "\n",
    "                                # Se desejado, aplica um resize\n",
    "                                if size > 0:\n",
    "                                    face = face.resize((size, size))\n",
    "                                    \n",
    "                                # face_count serve para não ocorrer sobrescrição de mais de um rosto por frame\n",
    "                                face_count += 1\n",
    "                                \n",
    "                                # Cria o path para o rosto atual\n",
    "                                \n",
    "                                path = class_path.joinpath(f\"{video_path.parts[-2]} {video_path.parts[-1]} {face_count} .jpg\")\n",
    "                                # Salva o rosto na pasta correta.\n",
    "                                face.save(path)\n",
    "\n",
    "                # Após o batch ser aplicado, resetamos a lista\n",
    "                frames = []\n",
    "\n",
    "        # Solta o objeto do VideoCapture\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instaciamos o objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropsaver = CropSaver(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testamos agora para apenas 1 vídeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good! Go check it!\n"
     ]
    }
   ],
   "source": [
    "cropsaver.callCropFaces(call=\"test-video\", check_every_frame=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora para um diretório inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropsaver.callCropFaces(call=\"test-folder\", check_every_frame=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4° Passo: Utilizar o objeto criado e iterar em todo o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E iniciamos o loop que pode levar de algumas horas até alguns dias para terminar, dependendo do hardware. Valores importante que devem ser setados nessa etapa por fim:\n",
    "- `check_every_frame`: Define de quantos em quantos frames será realizada a inferência da rede para obter o rosto. Caso o valor seja 1 a rede realizará a inferência de 1 em 1 frame, ou seja, tentará encontrar e recortar os rostos de todos os frames de todos os vídeos. Esse processo pode ser extremamente custoso. Como são aproximadamente 100 mil vídeos de aproximadamente 10 segundos cada, a checagem de 1 em 1 frame resultaria em aproximadamente 30 milhões de imagens (considerando a taxa dos vídeos de 30 frames por segunda). Essa quantidade de imagens é mais de 20 vezes o dataset do ImageNet. Aqui a sugestão está deixar esse valor como 30 (de 1 em 1 segundo) ou 15 (de meio em meio segundo).\n",
    "\n",
    "- `batch_size`: Define a quantidade máxima de imagens concatenadas que deve ser armazenada antes de realizar a inferência da MTCNN pela placa de vídeo. Para um hardware de 8 GB de memória de vídeo, o tamanho máximo de imagens está em torno de 80. Aqui o valor está setado pela metade uma vez que a quantidade máxima esperada de imagens concatenadas é aproximadamente 20 imagens, deixando uma margem.\n",
    "\n",
    "- `padding`: Define a área extra que deve ser extraída considerada na hora de extrair um rosto. Para aproveitar um pouco mais de informação, foi utilizado o valor 15.\n",
    "\n",
    "- `size`: Define um tamanho padrão de saída para as imagens. Como pode existir a necessidade de se alterar esse tamanho como desejar, o valor não é fornecido, extraindo a imagem em seu tamanho original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271505d8125a4255b1bd1b717a2e444c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_0...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eecee9b2ca347709957bd7c1464d6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "C:\\Users\\chris\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  boxes = np.array(boxes)\n",
      "C:\\Users\\chris\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  probs = np.array(probs)\n",
      "C:\\Users\\chris\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  points = np.array(points)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_1...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7052db20f3e74f1a9ea090356afc4785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_2...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc6371711d1466f9d35a0f1d273482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1748 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_3...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9fb8ac00f945f4a23c19e46910debd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_4...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51eda1bd1b941f2959e1d2f890c5ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_5...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd447c38cbf485bb932f8b3144cf90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2483 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_6...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0d782c0f0a4228b70735f2c6176ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_7...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2cbb3d95234fe08218512ba8ea7bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_8...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bd3328e8284d34bc1b647b4fa9d3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_9...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac535d06b9c94dde9de838d6151beb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_10...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed2886217dc476f886c6b1d64a272bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_11...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca32f02b316a4fdb840fe719665295ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_12...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dec7c30ae9421a9b28852d7ed2de19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_13...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9569eefe7a8476a97de953a5ffe2ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3694 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_14...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbfbee4cc4846d08cac2ce58c9cc28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_15...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9f6a5d762d4a789435dcea8dfcbdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_16...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2fb336a387472b8ba059496588d64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2061 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_17...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c285a830c8cf4a228296139121d37e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_18...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066197902dca449caed5ef5fa2f80d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_19...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15fef3f66aa498c924b45bbf61ea614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_20...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c442d2c3db9b4654ada8c15457380456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_21...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5cfc70f4f04e1da0e026cfe3c2c48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_22...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9def3025c44588a079bf10fe8396c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_23...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0a154e8d0448358f2e152b680cec34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_24...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4bf13915da44f8ad6ee51b0a97826a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2786 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_25...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dc61b0edbb4a2e8cf96e2f5d830823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_26...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e2dad6a2864c7cbe43479674ec00d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_27...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bb71eb976f48ec988c8fdb770277e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_28...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99635b9a27e24e2eaf1f2ac8ff94ac28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_29...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cd6f452bea4a7db5c704ed39305315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_30...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be66dd24df543d9aec64c4c9d61f849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_31...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c081f3d5835e4fe0b28acc4efce94759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_32...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146b197b873c40dca6d70b6bbdb4833c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_33...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62170dd6232041209beedbe2882d0c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_34...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa40d502ffc24f2fb445425f8c2b7069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_35...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579757949d6e48f492d2322005137a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_36...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51988af2e964c239acf3b9035245706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_37...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffd0ef4676945daaa1574ee48d83dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_38...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d6c0b450d74802a7d28587463a4554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2477 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_39...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c38a9fc094044788b443602b36645a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_40...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0711848328424b7c8cee8a76e952f3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_41...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f36d3778a994e2983b85045402aaadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_42...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a056494c66043909d1a6c432a200ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_43...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7b476c389242e9a76997d607179f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-35805a92e3e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcropsaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallCropFaces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_every_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-f72c61572fec>\u001b[0m in \u001b[0;36mcallCropFaces\u001b[1;34m(self, path_to_save, call, check_every_frame, batch_size, padding, size)\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0msplit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                     self.saveCropFaces(video_path, path_to_save, split, label, batch_size=batch_size,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                        padding=padding, size=size, check_every_frame=check_every_frame)\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-f72c61572fec>\u001b[0m in \u001b[0;36msaveCropFaces\u001b[1;34m(self, video_path, path_to_save, split, label, batch_size, padding, size, check_every_frame)\u001b[0m\n\u001b[0;32m    127\u001b[0m                                 \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{video_path.parts[-2]} {video_path.parts[-1]} {face_count} .jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                                 \u001b[1;31m# Salva o rosto na pasta correta.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                                 \u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;31m# Após o batch ser aplicado, resetamos a lista\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2159\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2161\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2163\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cropsaver.callCropFaces(call=\"all\", check_every_frame=15, padding=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_43...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cropsaver.folders = cropsaver.folders[43:44]\n",
    "\n",
    "cropsaver.callCropFaces(call=\"all\", path_to_save=\"./Kaggle Faces Dataset Extended\", check_every_frame=15, padding=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7251a9d21bc24eb4a141130641c0d846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_44...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268e7c9e0da4472ebc9d36411584bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_45...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452cd288474f4bdcbdd9c52a28eec317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_46...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d599f48ae3c4713ac4d6872f3be4340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_47...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3851fbf54f4d1cafe07ce5652f1d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_48...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adcd3160891488bac59d01b0ad8352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2463 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Beggining folder dfdc_train_part_49...--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcce050097e463396fd4354d72d8c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cropsaver.folders = cropsaver.folders[44:]\n",
    "\n",
    "cropsaver.callCropFaces(call=\"all\", path_to_save=\"./Kaggle Faces Dataset Extended\", check_every_frame=15, padding=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin = Path(\"./Kaggle Faces Dataset Extended/train/FAKE\")\n",
    "origin.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest = Path(\"./Kaggle Faces Dataset/train/FAKE\")\n",
    "dest.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e228f30629a41c4b05717307ba60e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen = origin.glob(\"*.jpg\")\n",
    "\n",
    "for file_path in tqdm(gen, total=231749):\n",
    "    if dest.joinpath(file_path.name).exists(): # Checa se existe a versão equivalente no outro folder\n",
    "        file_path.unlink() # Se ela existir, delete o item original apenas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
