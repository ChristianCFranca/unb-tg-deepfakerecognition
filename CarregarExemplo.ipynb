{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "promotional-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Para adquirir o modelo pré-treinado\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Para iterar pelos vídeos e tornar interativo se desejado\n",
    "import cv2\n",
    "\n",
    "# Para extrair os rostos dos frames\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# Útil para realizar operações em vetores\n",
    "import numpy as np\n",
    "\n",
    "# Dicionario para contabilizar os FAKE e os REAL\n",
    "from collections import defaultdict\n",
    "\n",
    "# Limpa prints extras nas células\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-quality",
   "metadata": {},
   "source": [
    "Define-se o dispositivo que irá rodar as computações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimensional-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos um device onde os tensores estarão sendo processados\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blind-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações para a MTCNN\n",
    "IMAGE_SIZE = 224\n",
    "MARGIN = 0\n",
    "MIN_FACE_SIZE = 90\n",
    "THRESHOLDS = [0.68, 0.75, 0.80]\n",
    "POST_PROCESS = False\n",
    "SELECT_LARGEST = True\n",
    "KEEP_ALL = False\n",
    "DEVICE = device\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "mtcnn = MTCNN(image_size=IMAGE_SIZE,\n",
    "              margin=MARGIN, \n",
    "              min_face_size=MIN_FACE_SIZE, \n",
    "              thresholds=THRESHOLDS,\n",
    "              post_process=POST_PROCESS,\n",
    "              select_largest=SELECT_LARGEST, \n",
    "              keep_all=KEEP_ALL, \n",
    "              device=device)\n",
    "\n",
    "path_to_learner = Path('./models/final_learner.pkl')\n",
    "learner = load_learner(path_to_learner, cpu=device.type == 'cpu') # As inferências nas imagens serão feitas pela CPU uma vez que será feito vídeo por vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-village",
   "metadata": {},
   "source": [
    "Define-se uma função que extrai os rostos dos vídeos utilizando a MTCNN e os guarda em lista lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "passing-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_video(video_path, padding=0, resize_factor=0.6, check_every_frame=30, show_frames=False):\n",
    "    \n",
    "    # Captura o vídeo no path\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "    except:\n",
    "        print(\"Ocorreu um erro ao carregar o vídeo. Certifique-se de que é um arquivo de vídeo válido.\")\n",
    "        return []\n",
    "    \n",
    "    # Pega, em inteiros, a quantidade de frames do vídeo\n",
    "    v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    faces = []\n",
    "    for i in range(1, v_len + 1):\n",
    "        success = cap.grab()\n",
    "        # Apertar a tecla 'q' para sair do vídeo.\n",
    "        if not success:\n",
    "            continue\n",
    "        if  i % check_every_frame == 0:\n",
    "            success, frame = cap.retrieve()\n",
    "            if not success:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if success: # Sucesso na leitura\n",
    "            # Obtém o frame como PIL Image (ele é capturado no formato BGR porém a MTCNN espera no formato RGB)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = Image.fromarray(frame_rgb)\n",
    "            if show_frames:\n",
    "                frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "                cv2.imshow('frame', frame)\n",
    "\n",
    "            boxes, _ = mtcnn.detect(frame_rgb) # Detecta as imagens. O método detect só aceita numpy arrays\n",
    "            \n",
    "            if boxes is not None: # Só entra se rostos forem detectados\n",
    "                for box in boxes: # Para cada uma das bouding boxes encontradas em um único frame (a princípio só deve ter uma)\n",
    "                    box = [int(b) for b in box]\n",
    "                    # Extrai a face\n",
    "                    face = frame_rgb.crop(box=(box[0]-padding, \n",
    "                                               box[1]-padding, \n",
    "                                               box[2]+padding, \n",
    "                                               box[3]+padding))\n",
    "                    faces.append(PILImage(face))\n",
    "\n",
    "                    if show_frames:\n",
    "                        frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "                        cv2.imshow('frame', frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    \n",
    "                \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cooked-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_with_detection(video_path, preds, padding=0, resize_factor=0.6, check_every_frame=30):\n",
    "    \n",
    "    # Captura o vídeo no path\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "    except:\n",
    "        print(\"Ocorreu um erro ao carregar o vídeo. Certifique-se de que é um arquivo de vídeo válido.\")\n",
    "        return []\n",
    "    \n",
    "    # Pega, em inteiros, a quantidade de frames do vídeo\n",
    "    v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    pred_idx = 0\n",
    "    \n",
    "    faces = []\n",
    "    for i in range(1, v_len + 1):\n",
    "        ret, frame = cap.read()\n",
    "        if  i % check_every_frame == 0:\n",
    "            # Apertar a tecla 'q' para sair do vídeo.\n",
    "            if not ret:\n",
    "                continue\n",
    "        else:\n",
    "            frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "            cv2.imshow('frame', frame)\n",
    "            continue\n",
    "        # Obtém o frame como PIL Image (ele é capturado no formato BGR porém a MTCNN espera no formato RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        boxes, _ = mtcnn.detect(frame_rgb) # Detecta as imagens. O método detect só aceita numpy arrays\n",
    "\n",
    "        if boxes is not None: # Só entra se rostos forem detectados\n",
    "            for box in boxes: # Para cada uma das bouding boxes encontradas em um único frame (a princípio só deve ter uma)\n",
    "                box = [int(b) for b in box]\n",
    "                # Extrai a face\n",
    "                face = frame_rgb.crop(box=(box[0]-padding, \n",
    "                                           box[1]-padding, \n",
    "                                           box[2]+padding, \n",
    "                                           box[3]+padding))\n",
    "                faces.append(PILImage(face))\n",
    "                \n",
    "                #frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "                coord_1 = (box[0]-padding, box[1]-padding)\n",
    "                coord_2 = (box[2]+padding, box[3]+padding)\n",
    "                cv2.rectangle(frame, coord_1, coord_2, (0, 255, 0), 3)\n",
    "                \n",
    "                text = \"FAKE\" if preds[pred_idx] == 0 else \"REAL\"\n",
    "                text_color =  (0, 0, 255) if preds[pred_idx] == 0 else (0, 255, 0)\n",
    "                cv2.putText(frame, text, (box[0], box[1]-7), cv2.FONT_HERSHEY_SIMPLEX, 1.5, text_color, 2, 2)\n",
    "                pred_idx += 1\n",
    "              \n",
    "        frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "        cv2.imshow('frame', frame)\n",
    "                \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intimate-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(learner, faces, verbose=False):\n",
    "    predicts = []\n",
    "    predicts_dict = defaultdict(lambda: 0)\n",
    "    for i, face in enumerate(faces):\n",
    "        res = learner.predict(face)\n",
    "        clear_output()\n",
    "        if verbose:\n",
    "            print(f\"Predição realizada para a face {i+1}\")\n",
    "        predicts.append(res[1].item())\n",
    "        predicts_dict[res[0]] += 1\n",
    "    print('-'*100)\n",
    "    print(F\"Resultados individuais: Quantidade de \\033[91m FAKES \\033[0m: {predicts_dict['FAKE']} | Quantidade de \\033[92m REALS \\033[0m: {predicts_dict['REAL']}\")\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "balanced-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_prediction_from_predictions(predictions, roh=2.75):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Utilizando regra com ρ = {roh}...\\n\")\n",
    "    if not isinstance(predictions, np.ndarray):\n",
    "        predictions = np.array(predictions)\n",
    "    qtd_fakes = np.count_nonzero(predictions == 0)\n",
    "    qtd_reals = np.count_nonzero(predictions == 1)\n",
    "    \n",
    "    return 'FAKE' if qtd_fakes >= roh*qtd_reals else 'REAL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-stockholm",
   "metadata": {},
   "source": [
    "### Teste visual rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "distinct-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"train_sample_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "color-pierce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etejaapnxh.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wtreibcmgm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etmcruaihe.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>afoovlsmtx.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etohcvnzbj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>bdnaqemxmr.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eudeqjhdfd.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eukvucdetx.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>gjypopglvi.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL  train            None\n",
       "abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4\n",
       "...              ...    ...             ...\n",
       "etejaapnxh.mp4  FAKE  train  wtreibcmgm.mp4\n",
       "etmcruaihe.mp4  FAKE  train  afoovlsmtx.mp4\n",
       "etohcvnzbj.mp4  FAKE  train  bdnaqemxmr.mp4\n",
       "eudeqjhdfd.mp4  REAL  train            None\n",
       "eukvucdetx.mp4  FAKE  train  gjypopglvi.mp4\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_json(base_path/\"metadata.json\").transpose()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "viral-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = random.choice(metadata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "viral-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predição realizada para a face 100\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Resultados individuais: Quantidade de \u001b[91m FAKES \u001b[0m: 100 | Quantidade de \u001b[92m REALS \u001b[0m: 0\n"
     ]
    }
   ],
   "source": [
    "check_every_frame = 3\n",
    "path = base_path/video_name\n",
    "\n",
    "faces = extract_faces_from_video(path, check_every_frame=check_every_frame)\n",
    "preds = get_predictions(learner, faces, metadata.loc[video_name].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "extreme-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video_with_detection(path, preds, resize_factor=0.6, check_every_frame=check_every_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-coral",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "russian-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('test_videos/aassnaulhq.mp4'),\n",
       " Path('test_videos/aayfryxljh.mp4'),\n",
       " Path('test_videos/acazlolrpz.mp4'),\n",
       " Path('test_videos/adohdulfwb.mp4'),\n",
       " Path('test_videos/ahjnxtiamx.mp4')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"test_videos\"\n",
    "list_of_paths = list(Path(folder).glob(\"*.mp4\"))[:10] # 10 primeiros vídeos\n",
    "list_of_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "usual-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Resultados individuais: Quantidade de \u001b[91m FAKES \u001b[0m: 0 | Quantidade de \u001b[92m REALS \u001b[0m: 10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Utilizando regra com ρ = 2.75...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_every_frame=30\n",
    "rho=2.75\n",
    "\n",
    "tempo_corrido = []\n",
    "qtde_rostos = []\n",
    "for path in list_of_paths:\n",
    "    inicio = time.time()\n",
    "    \n",
    "    faces = extract_faces_from_video(path, check_every_frame=check_every_frame)\n",
    "    if len(faces) == 0:\n",
    "        print(\"Não foi possível detectar faces humanas no vídeo fornecido.\")\n",
    "    else:\n",
    "        preds = get_predictions(learner, faces)\n",
    "        final_res = get_final_prediction_from_predictions(preds, rho)\n",
    "    \n",
    "    final = time.time()\n",
    "\n",
    "    tempo_corrido.append((final-inicio)) # Pegamos o tempo total e dividimos pela quantidade de faces\n",
    "    qtde_rostos.append(len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "public-recognition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PILImage mode=RGB size=177x210,\n",
       " PILImage mode=RGB size=153x208,\n",
       " PILImage mode=RGB size=160x202,\n",
       " PILImage mode=RGB size=150x203,\n",
       " PILImage mode=RGB size=158x222,\n",
       " PILImage mode=RGB size=163x209,\n",
       " PILImage mode=RGB size=163x237,\n",
       " PILImage mode=RGB size=161x221,\n",
       " PILImage mode=RGB size=161x207,\n",
       " PILImage mode=RGB size=181x240]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_faces_from_video(path, check_every_frame=check_every_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moderate-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_corrido = np.array(tempo_corrido)\n",
    "qtde_rostos = np.array(qtde_rostos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conventional-dodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.25596809, 3.27551985, 5.43652892, 3.02479959, 3.08222365,\n",
       "        5.34700036, 3.03099775, 3.01900196, 2.80803919, 3.20800185]),\n",
       " array([ 9, 10, 20, 10, 10, 20, 10, 10,  9, 10]),\n",
       " array([361.77423265, 327.55198479, 271.82644606, 302.47995853,\n",
       "        308.22236538, 267.35001802, 303.09977531, 301.90019608,\n",
       "        312.00435427, 320.8001852 ]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempo_corrido, qtde_rostos, tempo_corrido*1000/qtde_rostos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "inappropriate-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.5488081216812133, 0.9307750851079862)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempo_corrido.mean(), tempo_corrido.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-denver",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
