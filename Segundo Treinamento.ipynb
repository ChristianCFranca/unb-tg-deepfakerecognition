{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import io\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega uma rede ResNet18 pr√© treinada no ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "DeepFakeDataset = torchvision.datasets.ImageFolder('./Faces Dataset/224px/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "percentage_for_train = 0.7\n",
    "\n",
    "shuffle_indices = torch.randperm(len(DeepFakeDataset))\n",
    "train_indices = shuffle_indices[:int(percentage_for_train*len(DeepFakeDataset))]\n",
    "val_indices = shuffle_indices[int(percentage_for_train*len(DeepFakeDataset)):]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(DeepFakeDataset, batch_size=32, sampler=train_sampler)\n",
    "test_dataloader = torch.utils.data.DataLoader(DeepFakeDataset, batch_size=32, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def training_loop(n_epochs, model, loss_function, optimizer):\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss_sum = 0\n",
    "        iteration = 0\n",
    "        print(\"Beggining epoch {}...\".format(epoch+1))\n",
    "        for images, labels in train_dataloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss = loss_function(predictions, labels.long())\n",
    "            loss_sum += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 20 == 0:\n",
    "                print(\"Iteration {} Loss {:.5f}\".format(iteration, loss_sum.item()/20))\n",
    "                loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialmente, rodaremos o modelo cru no dataset para checar seu desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model):\n",
    "    all_labels = torch.LongTensor([]).cuda()\n",
    "    all_predictions = torch.LongTensor([]).cuda()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(images)\n",
    "            predictions = predictions.max(dim=1)[1]\n",
    "            all_predictions = torch.cat((all_predictions, predictions))\n",
    "            all_labels = torch.cat((all_labels, labels))\n",
    "            \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame_conf_matrix(all_pread, all_labels):\n",
    "    c_mat = pd.DataFrame(metrics.confusion_matrix(all_labels.cpu().detach().numpy(), all_pred.cpu().detach().numpy()))\n",
    "    c_mat = c_mat.rename(columns={0: 'FAKE_predicted', 1: \"REAL_predicted\"}, index={0: 'FAKE', 1: \"REAL\"})\n",
    "    rows = [c_mat.loc['FAKE'][0] / c_mat.sum()[0] * 100, c_mat.loc['REAL'][1] / c_mat.sum()[1] * 100]\n",
    "    columns = [c_mat['FAKE_predicted'][0] / c_mat.sum(axis=1)[0] * 100, c_mat['REAL_predicted'][1] / c_mat.sum(axis=1)[1] * 100]\n",
    "    accuracy = (c_mat.loc['FAKE'][0] + c_mat.loc['REAL'][1]) / (c_mat.sum(axis=1)[0] + c_mat.sum(axis=1)[1])*100 \n",
    "    c_mat.loc['Percentage'] = [str(round(rows[0],2)) +' %', str(round(rows[1],2)) +' %']\n",
    "    c_mat['Percentage'] = [str(round(columns[0],2)) +' %', str(round(columns[1],2)) +' %', str(round(accuracy,2)) +' %']\n",
    "    return c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = all_pred.to(dtype=torch.int8)\n",
    "all_labels = all_labels.to(dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>12103</td>\n",
       "      <td>162</td>\n",
       "      <td>98.68 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>6499</td>\n",
       "      <td>210</td>\n",
       "      <td>3.13 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>65.06 %</td>\n",
       "      <td>56.45 %</td>\n",
       "      <td>64.89 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                12103            162    98.68 %\n",
       "REAL                 6499            210     3.13 %\n",
       "Percentage        65.06 %        56.45 %    64.89 %"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora, treinamos o modelo e testamos novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.67643\n",
      "Iteration 40 Loss 0.65845\n",
      "Iteration 60 Loss 0.66034\n",
      "Iteration 80 Loss 0.64786\n",
      "Iteration 100 Loss 0.63336\n",
      "Iteration 120 Loss 0.63432\n",
      "Iteration 140 Loss 0.62661\n",
      "Iteration 160 Loss 0.62194\n",
      "Iteration 180 Loss 0.59436\n",
      "Iteration 200 Loss 0.62452\n",
      "Iteration 220 Loss 0.60225\n",
      "Iteration 240 Loss 0.61013\n",
      "Iteration 260 Loss 0.61110\n",
      "Iteration 280 Loss 0.59350\n",
      "Iteration 300 Loss 0.59040\n",
      "Iteration 320 Loss 0.58194\n",
      "Iteration 340 Loss 0.58583\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.58576\n",
      "Iteration 40 Loss 0.58914\n",
      "Iteration 60 Loss 0.58015\n",
      "Iteration 80 Loss 0.56742\n",
      "Iteration 100 Loss 0.57734\n",
      "Iteration 120 Loss 0.56756\n",
      "Iteration 140 Loss 0.57612\n",
      "Iteration 160 Loss 0.54798\n",
      "Iteration 180 Loss 0.56418\n",
      "Iteration 200 Loss 0.56996\n",
      "Iteration 220 Loss 0.57416\n",
      "Iteration 240 Loss 0.57541\n",
      "Iteration 260 Loss 0.56175\n",
      "Iteration 280 Loss 0.56356\n",
      "Iteration 300 Loss 0.56534\n",
      "Iteration 320 Loss 0.56293\n",
      "Iteration 340 Loss 0.55630\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.56275\n",
      "Iteration 40 Loss 0.56207\n",
      "Iteration 60 Loss 0.55060\n",
      "Iteration 80 Loss 0.54214\n",
      "Iteration 100 Loss 0.54087\n",
      "Iteration 120 Loss 0.53913\n",
      "Iteration 140 Loss 0.55167\n",
      "Iteration 160 Loss 0.55006\n",
      "Iteration 180 Loss 0.55654\n",
      "Iteration 200 Loss 0.54005\n",
      "Iteration 220 Loss 0.55786\n",
      "Iteration 240 Loss 0.53936\n",
      "Iteration 260 Loss 0.53492\n",
      "Iteration 280 Loss 0.54099\n",
      "Iteration 300 Loss 0.55904\n",
      "Iteration 320 Loss 0.53747\n",
      "Iteration 340 Loss 0.52063\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.52356\n",
      "Iteration 40 Loss 0.52971\n",
      "Iteration 60 Loss 0.53135\n",
      "Iteration 80 Loss 0.54012\n",
      "Iteration 100 Loss 0.54560\n",
      "Iteration 120 Loss 0.53001\n",
      "Iteration 140 Loss 0.52491\n",
      "Iteration 160 Loss 0.53397\n",
      "Iteration 180 Loss 0.54046\n",
      "Iteration 200 Loss 0.52991\n",
      "Iteration 220 Loss 0.52917\n",
      "Iteration 240 Loss 0.53029\n",
      "Iteration 260 Loss 0.53438\n",
      "Iteration 280 Loss 0.53549\n",
      "Iteration 300 Loss 0.52045\n",
      "Iteration 320 Loss 0.54157\n",
      "Iteration 340 Loss 0.54018\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>10911</td>\n",
       "      <td>1354</td>\n",
       "      <td>88.96 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>3632</td>\n",
       "      <td>3077</td>\n",
       "      <td>45.86 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>75.03 %</td>\n",
       "      <td>69.44 %</td>\n",
       "      <td>73.72 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                10911           1354    88.96 %\n",
       "REAL                 3632           3077    45.86 %\n",
       "Percentage        75.03 %        69.44 %    73.72 %"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.72256\n",
      "Iteration 40 Loss 0.71473\n",
      "Iteration 60 Loss 0.70292\n",
      "Iteration 80 Loss 0.68108\n",
      "Iteration 100 Loss 0.67987\n",
      "Iteration 120 Loss 0.67782\n",
      "Iteration 140 Loss 0.67009\n",
      "Iteration 160 Loss 0.66080\n",
      "Iteration 180 Loss 0.65615\n",
      "Iteration 200 Loss 0.65454\n",
      "Iteration 220 Loss 0.64711\n",
      "Iteration 240 Loss 0.63942\n",
      "Iteration 260 Loss 0.63260\n",
      "Iteration 280 Loss 0.63503\n",
      "Iteration 300 Loss 0.62150\n",
      "Iteration 320 Loss 0.63241\n",
      "Iteration 340 Loss 0.62298\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.62177\n",
      "Iteration 40 Loss 0.61201\n",
      "Iteration 60 Loss 0.60821\n",
      "Iteration 80 Loss 0.61511\n",
      "Iteration 100 Loss 0.60929\n",
      "Iteration 120 Loss 0.60121\n",
      "Iteration 140 Loss 0.60564\n",
      "Iteration 160 Loss 0.61737\n",
      "Iteration 180 Loss 0.60660\n",
      "Iteration 200 Loss 0.59891\n",
      "Iteration 220 Loss 0.60599\n",
      "Iteration 240 Loss 0.59215\n",
      "Iteration 260 Loss 0.60073\n",
      "Iteration 280 Loss 0.58796\n",
      "Iteration 300 Loss 0.58770\n",
      "Iteration 320 Loss 0.60131\n",
      "Iteration 340 Loss 0.57767\n",
      "Beggining epoch 3...\n",
      "Iteration 20 Loss 0.58587\n",
      "Iteration 40 Loss 0.58701\n",
      "Iteration 60 Loss 0.58739\n",
      "Iteration 80 Loss 0.58681\n",
      "Iteration 100 Loss 0.58822\n",
      "Iteration 120 Loss 0.56957\n",
      "Iteration 140 Loss 0.57823\n",
      "Iteration 160 Loss 0.57085\n",
      "Iteration 180 Loss 0.58520\n",
      "Iteration 200 Loss 0.57391\n",
      "Iteration 220 Loss 0.57645\n",
      "Iteration 240 Loss 0.58637\n",
      "Iteration 260 Loss 0.55957\n",
      "Iteration 280 Loss 0.57093\n",
      "Iteration 300 Loss 0.57630\n",
      "Iteration 320 Loss 0.56367\n",
      "Iteration 340 Loss 0.57352\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 3x maior para a classe de n√£o deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 2.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(n_epochs=3, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>7213</td>\n",
       "      <td>5052</td>\n",
       "      <td>58.81 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>1267</td>\n",
       "      <td>5442</td>\n",
       "      <td>81.11 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>85.06 %</td>\n",
       "      <td>51.86 %</td>\n",
       "      <td>66.7 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                 7213           5052    58.81 %\n",
       "REAL                 1267           5442    81.11 %\n",
       "Percentage        85.06 %        51.86 %     66.7 %"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.57211\n",
      "Iteration 40 Loss 0.56457\n",
      "Iteration 60 Loss 0.55785\n",
      "Iteration 80 Loss 0.56494\n",
      "Iteration 100 Loss 0.56019\n",
      "Iteration 120 Loss 0.56736\n",
      "Iteration 140 Loss 0.56689\n",
      "Iteration 160 Loss 0.57515\n",
      "Iteration 180 Loss 0.56002\n",
      "Iteration 200 Loss 0.56545\n",
      "Iteration 220 Loss 0.55541\n",
      "Iteration 240 Loss 0.54879\n",
      "Iteration 260 Loss 0.57667\n",
      "Iteration 280 Loss 0.55515\n",
      "Iteration 300 Loss 0.55830\n",
      "Iteration 320 Loss 0.57207\n",
      "Iteration 340 Loss 0.55743\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.55767\n",
      "Iteration 40 Loss 0.56051\n",
      "Iteration 60 Loss 0.56229\n",
      "Iteration 80 Loss 0.55561\n",
      "Iteration 100 Loss 0.55647\n",
      "Iteration 120 Loss 0.54622\n",
      "Iteration 140 Loss 0.55436\n",
      "Iteration 160 Loss 0.54590\n",
      "Iteration 180 Loss 0.55158\n",
      "Iteration 200 Loss 0.56509\n",
      "Iteration 220 Loss 0.54617\n",
      "Iteration 240 Loss 0.54703\n",
      "Iteration 260 Loss 0.55660\n",
      "Iteration 280 Loss 0.54843\n",
      "Iteration 300 Loss 0.55484\n",
      "Iteration 320 Loss 0.54946\n",
      "Iteration 340 Loss 0.53637\n",
      "Beggining epoch 3...\n",
      "Iteration 20 Loss 0.54547\n",
      "Iteration 40 Loss 0.55025\n",
      "Iteration 60 Loss 0.55286\n",
      "Iteration 80 Loss 0.53916\n",
      "Iteration 100 Loss 0.54275\n",
      "Iteration 120 Loss 0.53835\n",
      "Iteration 140 Loss 0.55328\n",
      "Iteration 160 Loss 0.54473\n",
      "Iteration 180 Loss 0.55470\n",
      "Iteration 200 Loss 0.52913\n",
      "Iteration 220 Loss 0.54391\n",
      "Iteration 240 Loss 0.55378\n",
      "Iteration 260 Loss 0.54117\n",
      "Iteration 280 Loss 0.54565\n",
      "Iteration 300 Loss 0.55226\n",
      "Iteration 320 Loss 0.54966\n",
      "Iteration 340 Loss 0.54850\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=3, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>8341</td>\n",
       "      <td>3924</td>\n",
       "      <td>68.01 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>1583</td>\n",
       "      <td>5126</td>\n",
       "      <td>76.4 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>84.05 %</td>\n",
       "      <td>56.64 %</td>\n",
       "      <td>70.98 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                 8341           3924    68.01 %\n",
       "REAL                 1583           5126     76.4 %\n",
       "Percentage        84.05 %        56.64 %    70.98 %"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem melhor! Infelizmente nossa acur√°cia geral caiu, mas o modelo conseguiu predizer mais rostos reais como reais e mais rostos falsos como falsos. Ainda h√° um longo caminho a se percorrer. Por hora, vamos tentar diminuir um pouco o learning rate e aumentar o n√∫mero de √©pocas utilizando um modelo mais poderoso e observar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.68799\n",
      "Iteration 40 Loss 0.64818\n",
      "Iteration 60 Loss 0.61646\n",
      "Iteration 80 Loss 0.60473\n",
      "Iteration 100 Loss 0.61077\n",
      "Iteration 120 Loss 0.59524\n",
      "Iteration 140 Loss 0.58154\n",
      "Iteration 160 Loss 0.58411\n",
      "Iteration 180 Loss 0.57724\n",
      "Iteration 200 Loss 0.57432\n",
      "Iteration 220 Loss 0.56837\n",
      "Iteration 240 Loss 0.57737\n",
      "Iteration 260 Loss 0.55935\n",
      "Iteration 280 Loss 0.57288\n",
      "Iteration 300 Loss 0.55380\n",
      "Iteration 320 Loss 0.57015\n",
      "Iteration 340 Loss 0.55017\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.54942\n",
      "Iteration 40 Loss 0.54488\n",
      "Iteration 60 Loss 0.54837\n",
      "Iteration 80 Loss 0.54233\n",
      "Iteration 100 Loss 0.54652\n",
      "Iteration 120 Loss 0.54310\n",
      "Iteration 140 Loss 0.54536\n",
      "Iteration 160 Loss 0.54697\n",
      "Iteration 180 Loss 0.53969\n",
      "Iteration 200 Loss 0.54218\n",
      "Iteration 220 Loss 0.55252\n",
      "Iteration 240 Loss 0.54760\n",
      "Iteration 260 Loss 0.54816\n",
      "Iteration 280 Loss 0.53299\n",
      "Iteration 300 Loss 0.53091\n",
      "Iteration 320 Loss 0.52282\n",
      "Iteration 340 Loss 0.52542\n",
      "Beggining epoch 3...\n",
      "Iteration 20 Loss 0.51310\n",
      "Iteration 40 Loss 0.53436\n",
      "Iteration 60 Loss 0.55759\n",
      "Iteration 80 Loss 0.55121\n",
      "Iteration 100 Loss 0.52640\n",
      "Iteration 120 Loss 0.53503\n",
      "Iteration 140 Loss 0.52405\n",
      "Iteration 160 Loss 0.53048\n",
      "Iteration 180 Loss 0.52736\n",
      "Iteration 200 Loss 0.53422\n",
      "Iteration 220 Loss 0.52109\n",
      "Iteration 240 Loss 0.52668\n",
      "Iteration 260 Loss 0.53512\n",
      "Iteration 280 Loss 0.52301\n",
      "Iteration 300 Loss 0.50279\n",
      "Iteration 320 Loss 0.50962\n",
      "Iteration 340 Loss 0.53889\n"
     ]
    }
   ],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet50.fc.in_features\n",
    "\n",
    "resnet50.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 9x maior para a classe de n√£o deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 1.5]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet50.fc.parameters(), lr=0.5e-3)\n",
    "\n",
    "training_loop(n_epochs=3, model=resnet50, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>9331</td>\n",
       "      <td>2934</td>\n",
       "      <td>76.08 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>1802</td>\n",
       "      <td>4907</td>\n",
       "      <td>73.14 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>83.81 %</td>\n",
       "      <td>62.58 %</td>\n",
       "      <td>75.04 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                 9331           2934    76.08 %\n",
       "REAL                 1802           4907    73.14 %\n",
       "Percentage        83.81 %        62.58 %    75.04 %"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet50)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um resultado promissor. Embora nossa acur√°cia geral tenha aumentado apenas um pouco para 75.04%, fomos capazes de classificar corretamente 76.08% dos deepfakes e 73.14% dos reais. Estamos indo certo em alguma dire√ß√£o!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet50.state_dict(), './Saved Models/resnet50_pretrained_trainedover4folders_balanced.pt')\n",
    "\n",
    "#torch.save({\n",
    "#            'epoch': epoch,\n",
    "#            'model_state_dict': model.state_dict(),\n",
    "#            'optimizer_state_dict': optimizer.state_dict(),\n",
    "#            'loss': loss,\n",
    "#            ...\n",
    "#            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=False)\n",
    "    \n",
    "num_features = resnet50.fc.in_features\n",
    "\n",
    "resnet50.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "resnet50.load_state_dict(torch.load('./Saved Models/resnet50_pretrained_trainedover4folders_balanced.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-24da89811478>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_loop' is not defined"
     ]
    }
   ],
   "source": [
    "# Aqui estou experimentado colocar um peso 9x maior para a classe de n√£o deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 1.5]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet50.fc.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(n_epochs=5, model=resnet50, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>8954</td>\n",
       "      <td>3260</td>\n",
       "      <td>73.31 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>1426</td>\n",
       "      <td>5334</td>\n",
       "      <td>78.91 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>86.26 %</td>\n",
       "      <td>62.07 %</td>\n",
       "      <td>75.3 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                 8954           3260    73.31 %\n",
       "REAL                 1426           5334    78.91 %\n",
       "Percentage        86.26 %        62.07 %     75.3 %"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet50)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.59426\n",
      "Iteration 40 Loss 0.52556\n",
      "Iteration 60 Loss 0.41659\n",
      "Iteration 80 Loss 0.46110\n",
      "Iteration 100 Loss 0.36822\n",
      "Iteration 120 Loss 0.37085\n",
      "Iteration 140 Loss 0.35899\n",
      "Iteration 160 Loss 0.34524\n",
      "Iteration 180 Loss 0.29661\n",
      "Iteration 200 Loss 0.25882\n",
      "Iteration 220 Loss 0.23727\n",
      "Iteration 240 Loss 0.29944\n",
      "Iteration 260 Loss 0.27978\n",
      "Iteration 280 Loss 0.28345\n",
      "Iteration 300 Loss 0.23573\n",
      "Iteration 320 Loss 0.30361\n",
      "Iteration 340 Loss 0.24410\n",
      "Iteration 360 Loss 0.23524\n",
      "Iteration 380 Loss 0.23996\n",
      "Iteration 400 Loss 0.26933\n",
      "Iteration 420 Loss 0.21516\n",
      "Iteration 440 Loss 0.24700\n",
      "Iteration 460 Loss 0.15264\n",
      "Iteration 480 Loss 0.23808\n",
      "Iteration 500 Loss 0.22310\n",
      "Iteration 520 Loss 0.19420\n",
      "Iteration 540 Loss 0.21317\n",
      "Iteration 560 Loss 0.21707\n",
      "Iteration 580 Loss 0.17902\n",
      "Iteration 600 Loss 0.19321\n",
      "Iteration 620 Loss 0.22920\n",
      "Iteration 640 Loss 0.19598\n",
      "Iteration 660 Loss 0.19203\n",
      "Iteration 680 Loss 0.20343\n",
      "Iteration 700 Loss 0.16264\n",
      "Iteration 720 Loss 0.15339\n",
      "Iteration 740 Loss 0.14982\n",
      "Iteration 760 Loss 0.18803\n",
      "Iteration 780 Loss 0.21415\n",
      "Iteration 800 Loss 0.19019\n",
      "Iteration 820 Loss 0.16709\n",
      "Iteration 840 Loss 0.15560\n",
      "Iteration 860 Loss 0.13039\n",
      "Iteration 880 Loss 0.24338\n",
      "Iteration 900 Loss 0.14660\n",
      "Iteration 920 Loss 0.20467\n",
      "Iteration 940 Loss 0.15875\n",
      "Iteration 960 Loss 0.18639\n",
      "Iteration 980 Loss 0.17889\n",
      "Iteration 1000 Loss 0.15379\n",
      "Iteration 1020 Loss 0.14015\n",
      "Iteration 1040 Loss 0.19195\n",
      "Iteration 1060 Loss 0.11756\n",
      "Iteration 1080 Loss 0.14989\n",
      "Iteration 1100 Loss 0.14494\n",
      "Iteration 1120 Loss 0.11909\n",
      "Iteration 1140 Loss 0.14009\n",
      "Iteration 1160 Loss 0.13876\n",
      "Iteration 1180 Loss 0.14206\n",
      "Iteration 1200 Loss 0.15741\n",
      "Iteration 1220 Loss 0.14575\n",
      "Iteration 1240 Loss 0.15239\n",
      "Iteration 1260 Loss 0.14009\n",
      "Iteration 1280 Loss 0.12248\n",
      "Iteration 1300 Loss 0.10706\n",
      "Iteration 1320 Loss 0.12774\n",
      "Iteration 1340 Loss 0.15373\n",
      "Iteration 1360 Loss 0.13569\n",
      "Iteration 1380 Loss 0.15107\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.14227\n",
      "Iteration 40 Loss 0.10816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ab73565dd229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-4f0be7424239>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, model, loss_function, optimizer)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PytorchML\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet50.fc.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 1.5]))    \n",
    "optimizer = torch.optim.Adam(resnet50.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop(n_epochs=3, model=resnet50, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>11708</td>\n",
       "      <td>468</td>\n",
       "      <td>96.16 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>391</td>\n",
       "      <td>6407</td>\n",
       "      <td>94.25 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>96.77 %</td>\n",
       "      <td>93.19 %</td>\n",
       "      <td>95.47 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                11708            468    96.16 %\n",
       "REAL                  391           6407    94.25 %\n",
       "Percentage        96.77 %        93.19 %    95.47 %"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet50)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eita :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet50.state_dict(), './Saved Models/resnet50_pretrained_balanced_unfreezed.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
