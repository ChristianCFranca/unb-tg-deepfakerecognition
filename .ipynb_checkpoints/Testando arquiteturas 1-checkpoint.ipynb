{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos carregar algumas arquiteturas que foram treinadas e vamos testá-las no dataset de vídeos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos carregar um modelo ResNet18 treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=False)\n",
    "in_features = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(in_features, 2)\n",
    "\n",
    "resnet18.load_state_dict(torch.load('./Saved Models/resnet18_balanced_unfreezed_F97_R97_A97.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos um device onde a validação será rodada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Definimos um device onde os tensors estarão sendo processados\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos uma lista de todos as pastas que contém os vídeos para testar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dfdc_train_part_0',\n",
       " 'dfdc_train_part_1',\n",
       " 'dfdc_train_part_10',\n",
       " 'dfdc_train_part_11',\n",
       " 'dfdc_train_part_12']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria uma lista de todas as pastas disponíveis para treinamento\n",
    "folders = next(os.walk('./Kaggle Dataset/'))[1]\n",
    "folders[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos algumas classes e funções para nos auxiliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Videos():\n",
    "    def __init__(self, folder_path):\n",
    "        # Guarda o folder_path\n",
    "        self.folder_path = folder_path\n",
    "        \n",
    "        # Guarda a lista de todos os arquivos de videos dentro do folder_path\n",
    "        self.video_files = glob.glob(folder_path + '/*.mp4')\n",
    "        \n",
    "        # Lê o arquivo JSON que contém as informações dos deepfakes naquela pasta\n",
    "        self.metadata = pd.read_json(folder_path + '/metadata.json').transpose() # Essa transposiçao eh feita pois as colunas e as linhas estao trocadas\n",
    "        \n",
    "    def getRandomVideo(self):\n",
    "        video_path = random.choice(self.video_files)\n",
    "        video_name = os.path.basename(video_path)\n",
    "        label = self.metadata.loc[video_name].label\n",
    "        \n",
    "        return video_path, video_name, label\n",
    "        \n",
    "    def getRealVideo(self, video_name):\n",
    "        real_video_name = self.metadata.loc[video_name].original\n",
    "        # Verifica se é NaN, pois caso seja o nome original é o próprio video real\n",
    "        if pd.isna(real_video_name):\n",
    "            real_video_name = video_name\n",
    "        real_video_path = folder_path + '/' + real_video_name\n",
    "        return real_video_path, real_video_name, \"REAL\"\n",
    "    \n",
    "    def getAllVideosPath(self):\n",
    "        for video_name, columns in self.metadata.iterrows():\n",
    "            yield self.folder_path + '/' + video_name, video_name, columns[0] # Label\n",
    "\n",
    "def showVideo(video_path, label):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Configura a cor a ser colocada na LABEL\n",
    "    if label == 'REAL':\n",
    "        color = (0, 255, 0) # Verde\n",
    "    else:\n",
    "        color = (0, 0, 255) # Vermelho    \n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            boxes, _ = mtcnn.detect(Image.fromarray(frame))\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color=[0, 255, 0], thickness=5)\n",
    "                    # Escreve a label no vídeo sob a cabeça do indivíduo\n",
    "                    cv2.putText(img=frame, text=label, org=(box[0], box[1]), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=color, thickness=2)\n",
    "\n",
    "            frame = cv2.resize(frame, (1280, 720))\n",
    "            cv2.imshow('frame', frame)\n",
    "            #cv2.waitKey(10)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def predictVideo(video_path, padding=0, size=224, check_every_frame=15, model=resnet18):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    face = None\n",
    "    video_frame = 1\n",
    "    predictions = [0, 0]\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            if video_frame == 1 or video_frame % check_every_frame == 0:\n",
    "                boxes = None\n",
    "                boxes, _ = mtcnn.detect(Image.fromarray(frame))\n",
    "                if boxes is not None:\n",
    "                    for box in boxes: \n",
    "                        face = frame[\n",
    "                            int(max(box[1] - padding, 0)):int(max(box[3] + padding, 0)), \n",
    "                            int(max(box[0] - padding, 0)):int(max(box[2] + padding, 0))\n",
    "                        ]\n",
    "                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color=[0, 255, 0], thickness=5)\n",
    "                        \n",
    "                    # Predição\n",
    "                    prediction = model(getYUVTensor(face, size, transform).to(device)).max(dim=1)[1]\n",
    "                    predictions[prediction.item()] += 1\n",
    "            frame = cv2.resize(frame, (1280, 720))\n",
    "            #cv2.imshow('frame', frame)\n",
    "            video_frame += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return predictions\n",
    "\n",
    "def getYUVTensor(face, size, transform):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2YCrCb)\n",
    "    face = cv2.resize(face, (size, size))[:, :, 0]\n",
    "    face_tensor = transform(face)\n",
    "    face_tensor = torch.cat((face_tensor, face_tensor, face_tensor)).unsqueeze(0)\n",
    "    return face_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defini-se uma função getVeredict que recebe o vetor de predições 'Fake' e 'Real' e devolve o veredito.\n",
    "\n",
    "Nesse caso, o veredito é apenas a predição que está em maior quantidade no vídeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVeredict(predictions):\n",
    "    prediction_dict = {0: 'FAKE',\n",
    "                       1: 'REAL'}\n",
    "    veredict = prediction_dict[predictions.index(max(predictions))]\n",
    "    \n",
    "    #if  max(predictions) / (min(predictions) + max(predictions)) > 0.65:\n",
    "    #    return veredict\n",
    "    #else:\n",
    "    #    return \"Uncertain... Most to {}, i guess?\".format(veredict)\n",
    "    return veredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletamos uma pasta e vídeo aleatórios para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos a coleta de uma pasta aleatória\n",
    "import random\n",
    "random_folder = random.choice(folders) + '/'\n",
    "\n",
    "folder_path = './Kaggle Dataset/' + random_folder\n",
    "\n",
    "# Instaciamos uma classe de Videos\n",
    "videos = Videos(folder_path)\n",
    "\n",
    "mtcnn = MTCNN(keep_all=False, device=device, thresholds=[0.68, 0.78, 0.78], min_face_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra o vídeo para checagem visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path, video_name, label = videos.getRandomVideo()\n",
    "showVideo(video_path, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza as predições dele para cada 5 frames, devolve o vetor de predições e joga na função getVeredict para avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([61, 0], 'FAKE')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictVideo(video_path, padding=10, size=224, check_every_frame=5, model=resnet18)\n",
    "predictions, getVeredict(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora mostra o vídeo real do anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_video_path, real_video_name, label = videos.getRealVideo(video_name)\n",
    "showVideo(real_video_path, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E novamente realiza as predições em cima dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 53], 'REAL')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictVideo(real_video_path, padding=10, size=224, check_every_frame=5, model=resnet18)\n",
    "predictions, getVeredict(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop que valida todos os vídeos de todas as pastas do dataset do Kaggle e constrói uma matriz de confusão acumulativa das pastas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Início do folder dfdc_train_part_0 --------------\n",
      "dfdc_train_part_0: 0.00%...\n",
      "dfdc_train_part_0: 10.00%...\n",
      "dfdc_train_part_0: 20.00%...\n",
      "dfdc_train_part_0: 30.00%...\n",
      "dfdc_train_part_0: 40.00%...\n",
      "dfdc_train_part_0: 50.00%...\n",
      "dfdc_train_part_0: 60.00%...\n",
      "dfdc_train_part_0: 70.00%...\n",
      "dfdc_train_part_0: 80.00%...\n",
      "dfdc_train_part_0: 90.00%...\n",
      "dfdc_train_part_0: 100.00%...\n",
      "Accuracy: 97.60%\n",
      "      Predicted_Real  Predicted_Fake  Total\n",
      "REAL              79              25     86\n",
      "FAKE               7            1223   1248\n",
      "-------------- Início do folder dfdc_train_part_1 --------------\n",
      "dfdc_train_part_1: 0.00%...\n",
      "dfdc_train_part_1: 10.00%...\n",
      "dfdc_train_part_1: 20.00%...\n",
      "dfdc_train_part_1: 30.00%...\n",
      "dfdc_train_part_1: 40.00%...\n",
      "dfdc_train_part_1: 50.00%...\n",
      "dfdc_train_part_1: 60.00%...\n",
      "dfdc_train_part_1: 70.00%...\n",
      "dfdc_train_part_1: 80.00%...\n",
      "dfdc_train_part_1: 90.00%...\n",
      "dfdc_train_part_1: 99.00%...\n",
      "Accuracy: 98.78%\n",
      "      Predicted_Real  Predicted_Fake  Total\n",
      "REAL             185              28    194\n",
      "FAKE               9            2811   2839\n",
      "-------------- Início do folder dfdc_train_part_10 --------------\n",
      "dfdc_train_part_10: 0.00%...\n",
      "dfdc_train_part_10: 10.00%...\n",
      "dfdc_train_part_10: 20.00%...\n",
      "dfdc_train_part_10: 30.00%...\n",
      "dfdc_train_part_10: 40.00%...\n",
      "dfdc_train_part_10: 50.00%...\n",
      "dfdc_train_part_10: 60.00%...\n",
      "dfdc_train_part_10: 70.00%...\n",
      "dfdc_train_part_10: 80.00%...\n",
      "dfdc_train_part_10: 90.00%...\n",
      "dfdc_train_part_10: 100.00%...\n",
      "Accuracy: 97.67%\n",
      "      Predicted_Real  Predicted_Fake  Total\n",
      "REAL             489              61    573\n",
      "FAKE              84            5591   5652\n",
      "-------------- Início do folder dfdc_train_part_11 --------------\n",
      "dfdc_train_part_11: 0.00%...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fb918c106921>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: {:.2f}%...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_video\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mvideos_quantity\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#print(\"Video: {}\".format(video_data[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshowAndPredictVideo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_every_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvideo_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'FAKE'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-68eb59ca78b5>\u001b[0m in \u001b[0;36mshowAndPredictVideo\u001b[1;34m(video_path, padding, size, check_every_frame, model)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvideo_frame\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mvideo_frame\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcheck_every_frame\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mboxes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PytorchML\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m             )\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PytorchML\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001b[0m in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MTCNN batch processing only compatible with equal-dimension images.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mimgs_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PytorchML\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[0msl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN(keep_all=False, device=device, thresholds=[0.68, 0.78, 0.78], min_face_size=150)\n",
    "\n",
    "total_fake_videos = 0\n",
    "total_real_videos = 0\n",
    "correct_fake = 0\n",
    "correct_real = 0\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = './Kaggle Dataset/' + folder\n",
    "    videos = Videos(folder_path)\n",
    "    videos_generator = videos.getAllVideosPath()\n",
    "    print(\"-------------- Início do folder {} --------------\".format(folder))\n",
    "    videos_quantity = len(videos.video_files)\n",
    "    percentage = 10\n",
    "    print_every = int(videos_quantity / (100/percentage))\n",
    "\n",
    "    for n_video, video_data in enumerate(videos_generator):\n",
    "        if n_video % print_every == 0:\n",
    "            print(\"{}: {:.2f}%...\".format(folder, round(n_video / videos_quantity * 100)))\n",
    "        #print(\"Video: {}\".format(video_data[1]))\n",
    "        predictions = predictVideo(video_data[0], padding=10, size=224, check_every_frame=15, model=resnet18)\n",
    "        \n",
    "        if video_data[2] == 'FAKE':\n",
    "            total_fake_videos += 1\n",
    "            if getVeredict(predictions) == 'FAKE':\n",
    "                correct_fake += 1\n",
    "        else:\n",
    "            total_real_videos += 1\n",
    "            if getVeredict(predictions) == 'REAL':\n",
    "                correct_real += 1\n",
    "                \n",
    "    data = {\"Predicted_Real\": [correct_real, total_fake_videos - correct_fake], \"Predicted_Fake\": [total_real_videos - correct_real, correct_fake], \"Total\": [total_real_videos, total_fake_videos]}\n",
    "    accuracy = (correct_real + correct_fake) / (total_real_videos + total_fake_videos)\n",
    "    print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "    data = pd.DataFrame(data, index=['REAL', 'FAKE'])\n",
    "    print(data)\n",
    "    data.to_csv('./Saved .csv/videos_confusion_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_Real</th>\n",
       "      <th>Predicted_Fake</th>\n",
       "      <th>Total</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>490</td>\n",
       "      <td>84</td>\n",
       "      <td>574</td>\n",
       "      <td>85.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>61</td>\n",
       "      <td>5596</td>\n",
       "      <td>5657</td>\n",
       "      <td>98.92%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predicted_Real  Predicted_Fake  Total Accuracy\n",
       "REAL             490              84    574   85.37%\n",
       "FAKE              61            5596   5657   98.92%"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"Predicted_Real\": [correct_real, total_fake_videos - correct_fake], \"Predicted_Fake\": [total_real_videos - correct_real, correct_fake], \"Total\": [total_real_videos, total_fake_videos]}\n",
    "accuracy1 = \"{:.2f}%\".format(correct_real / total_real_videos*100)\n",
    "accuracy2 = \"{:.2f}%\".format(correct_fake / total_fake_videos*100)\n",
    "data['Accuracy'] = [accuracy1, accuracy2]\n",
    "accuracy = (correct_real + correct_fake) / (total_real_videos + total_fake_videos)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "data = pd.DataFrame(data, index=['REAL', 'FAKE'])\n",
    "data\n",
    "#data.to_csv('./Saved .csv/videos_confusion_matrix.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
