{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este notebook tem como função criar uma pasta chamada \"Faces Dataset\" onde os rostos recortados dos vídeos serão guardados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, carregamos as dependências necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodologia para criar o Image Dataset:\n",
    "- Inicialmente, achar bons valores de threshold para a detecção dos rostos, de forma que se garanta uma confiabilidade alta para todos os vídeos. (garantir que estou encontrando o rosto e estou recortando nos limiares aceitáveis). Podem haver imagens salvas que não são rostos, mas se o threshold estiver bem ajustado, não será suficiente para impactar no desempenho da nossa rede.\n",
    "- Teremos valores móveis para decidir de quantos em quantos frames a detecção será realizada e o rosto recortado salvo.\n",
    "\n",
    "Em seguida, seguir o seguinte loop de rotina:\n",
    "\n",
    "Para cada pasta do dataset:\n",
    "\n",
    "Para cada vídeo dentro da pasta:\n",
    "- Abrir o vídeo frame a frame.\n",
    "- Aplicar a detecção do rosto (com tamanho padrão definido) => Tamanho de entrada dos modelos pré treinados = 224x224 px\n",
    "- Recortar a região devolvida do rosto no canal RGB natural.\n",
    "- Salvar em uma pasta correspondente sendo FAKE ou REAL\n",
    "- Repetir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1° passo: Definir as funções e os objetos principais de acesso aos vídeos nas pastas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define-se o device onde será rodado a detecção de rostos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0, GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# Definimos um device onde os tensores estarão sendo processados\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}, {}'.format(device, torch.cuda.get_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dfdc_train_part_0',\n",
       " 'dfdc_train_part_1',\n",
       " 'dfdc_train_part_10',\n",
       " 'dfdc_train_part_11',\n",
       " 'dfdc_train_part_12']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria uma lista de todas as pastas disponíveis para treinamento\n",
    "folders = next(os.walk('./Kaggle Dataset/'))[1]\n",
    "folders[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Classe **Videos()**: Classe recebe um folder específico onde contém vídeos, e coleta todos os vídeos ali dentro presentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuida de lidar com o acesso aos vídeos e devolver os paths / labels corretamente\n",
    "class Videos():\n",
    "    def __init__(self, folder_path):\n",
    "        # Guarda o folder_path\n",
    "        self.folder_path = folder_path\n",
    "        \n",
    "        # Guarda a lista de todos os arquivos de videos dentro do folder_path\n",
    "        self.video_files = glob.glob(folder_path + '/*.mp4')\n",
    "        \n",
    "        # Lê o arquivo JSON que contém as informações dos deepfakes naquela pasta\n",
    "        self.metadata = pd.read_json(folder_path + '/metadata.json').transpose() # Essa transposiçao eh feita pois as colunas e as linhas estao trocadas\n",
    "        \n",
    "    def getRandomVideo(self):\n",
    "        video_path = random.choice(self.video_files)\n",
    "        video_name = os.path.basename(video_path)\n",
    "        label = self.metadata.loc[video_name].label\n",
    "        \n",
    "        return video_path, video_name, label\n",
    "    \n",
    "    def getRandomFakeVideo(self):\n",
    "        video_name = random.choice(videos.metadata[videos.metadata['label'] == 'FAKE'].index)\n",
    "        video_path = self.folder_path + '/' + video_name\n",
    "        \n",
    "        return video_path, video_name, 'FAKE'\n",
    "        \n",
    "    def getRealVideo(self, video_name):\n",
    "        \"\"\"\n",
    "            Recebe um `video_name` e a partir dele encontra o caminho para o vídeo original.\n",
    "        \"\"\"\n",
    "        real_video_name = self.metadata.loc[video_name].original\n",
    "        # Verifica se é NaN, pois caso seja o nome original é o próprio video real\n",
    "        if pd.isna(real_video_name):\n",
    "            real_video_name = video_name\n",
    "        real_video_path = self.folder_path + '/' + real_video_name\n",
    "        \n",
    "        return real_video_path, real_video_name, 'REAL'\n",
    "    \n",
    "    def getAllVideosPath(self):\n",
    "        for video_name, columns in self.metadata.iterrows():\n",
    "            yield self.folder_path + '/' + video_name, video_name, columns[0] # Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função **showVideo()**: Mostra o vídeo para checagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showVideo(video_path, label, padding=0, size=-1, channel=None, separate_face_box=False, resize_factor=0.6):\n",
    "    \n",
    "    # Captura o vídeo no path\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Configura a cor a ser colocada na LABEL\n",
    "    if label == 'REAL':\n",
    "        color = (0, 255, 0) # Verde\n",
    "    else:\n",
    "        color = (0, 0, 255) # Vermelho  \n",
    "    \n",
    "    face = None\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            boxes, _ = mtcnn.detect(Image.fromarray(frame))\n",
    "            if boxes is not None:\n",
    "                for box in boxes: \n",
    "                    face = frame[int(box[1] - padding):int(box[3] + padding), int(box[0] - padding):int(box[2] + padding)].copy()\n",
    "                    cv2.putText(img=frame, text=label, org=(box[0], box[1]), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=color, thickness=2)\n",
    "                    cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color=[0, 255, 0], thickness=5)\n",
    "                    if channel == 'luma':\n",
    "                        face = cv2.cvtColor(face, cv2.COLOR_BGR2YCrCb)\n",
    "                        face = face[:,:,0] # Pega apenas o canal Y\n",
    "                    \n",
    "            if face is not None:\n",
    "                if size > 0:\n",
    "                    face = cv2.resize(face, (size, size))\n",
    "                \n",
    "                if separate_face_box:\n",
    "                    cv2.imshow('face', face)\n",
    "                \n",
    "            frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "            cv2.imshow('frame', frame)\n",
    "            \n",
    "            # Apertar a tecla 'q' para sair do vídeo.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função **saveCropFaces()**: Recorta todos os rostos de um vídeo dada uma taxa de checagem por frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa é de fato a função que entra no loop e é responsável por recortar o rosto e o salvar \n",
    "# na pasta correspondente, para todos os frames do vídeo. Ela é chamada 1 vez por vídeo.\n",
    "def saveCropFaces(video_path, video_name, label, folder_name, batch_size=20, padding=0, size=224, check_every_frame=5, channel=None, folder='Dataset provisório', size_folder='no-resize-color'):\n",
    "    \n",
    "    # Instancia um VideoCapture do arquivo presente em video_path (no caso, o vídeo)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Pega, em inteiros, a quantidade de frames do vídeo\n",
    "    v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Cria um path para salvar os rostos recortados do vídeo. O path nesse caso está sendo './Faces Dataset/224px/FAKE ou REAL'\n",
    "    if size_folder:\n",
    "        class_path = './' + folder + '/' + size_folder + '/' + label\n",
    "    else:\n",
    "        class_path = './' + folder + '/' + label\n",
    "    \n",
    "    # Inicializa frames como uma lista vazia\n",
    "    frames = []\n",
    " \n",
    "    # face_count serve para não ocorrer sobrescrição de mais de um rosto por frame\n",
    "    face_count = 0\n",
    "\n",
    "    # Entra num loop que percorre o vídeo até ele acabar\n",
    "    for _ in range(1, v_len + 1):\n",
    "        # Realiza um grab() no próximo frame, mas não o decodifica. Isso ajuda a agilizar o processo se não for necessário\n",
    "        # recuperar todos os frames a todo o momento.\n",
    "        success = cap.grab()\n",
    "        # Só recorta o rosto se o frame atual for mod check_every_frame, ou seja, ele só decodifica de check_every_frame em check_every_frame frames.\n",
    "        if _ == 1 or _ % check_every_frame == 0:\n",
    "            success, frame = cap.retrieve()\n",
    "        else:\n",
    "            continue\n",
    "        if not success:\n",
    "            continue\n",
    "        # Realiza um append do frame atual na lista frames\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "        \n",
    "        if len(frames) >= batch_size or (_ == v_len and len(frames) > 0):\n",
    "            # Utiliza o MTCNN para detectar todas as bounding boxes de todos os rostos\n",
    "            boxes, probs = mtcnn.detect(frames)\n",
    "            # Verifica se não foi obtida nenhuma bounding box em todo o batch\n",
    "            if not all(x is None for x in boxes):\n",
    "                # Acessa cada um dos frames no batch\n",
    "                for i, boxes_f in enumerate(boxes):\n",
    "                    # Verifica houve None para o frame atual\n",
    "                    if boxes_f is not None:\n",
    "                        # Acessa cada uma das bounding boxes dentro de um único frame (pode haver vários rostos)\n",
    "                        for bbox in boxes_f:\n",
    "                            #face = np.array(frames[i])[\n",
    "                            #    int(max(bbox[1] - padding, 0)):int(max(bbox[3] + padding, 0)), \n",
    "                            #    int(max(bbox[0] - padding, 0)):int(max(bbox[2] + padding, 0))\n",
    "                            #]\n",
    "                            face = frames[i].crop(box=(bbox[0]-padding, \n",
    "                                                       bbox[1]-padding, \n",
    "                                                       bbox[2]+padding, \n",
    "                                                       bbox[3]+padding))\n",
    "                            \n",
    "                            # Se desejado, converte a região do rosto para YCrCb\n",
    "                            if channel == 'luma':\n",
    "                                face = cv2.cvtColor(np.array(face), cv2.COLOR_RGB2YCrCb)\n",
    "                                face = face[:,:,0] # Pega apenas o canal Y\n",
    "                                face = Image.fromarray(face)\n",
    "\n",
    "                            # Se desejado, aplica um resize\n",
    "                            if size > 0:\n",
    "                                face = face.resize((size, size))\n",
    "                            \n",
    "                            face_count += 1\n",
    "                            \n",
    "                            # Cria o path para o rosto atual\n",
    "                            path = os.path.join(class_path + f'/{folder_name} {video_name} {face_count}.jpg')\n",
    "                            # Salva o rosto na pasta correta. Observe que ele salva apenas face[:, :, 0] no caso do formato YCrCb (canal 'Y' = luma)\n",
    "                            face.save(path)\n",
    "\n",
    "            frames = []\n",
    "\n",
    "    # Solta o objeto do VideoCapture\n",
    "    cap.release()\n",
    "    # Destrói as janelas atualmente ativas\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2° passo: Observar alguns vídeos e ajustar o threshold do MTCNN para a detecção de rostos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MTCNN** - *Multi-task Cascaded Convolutional Network*\n",
    "\n",
    "Ajustamos os thresholds e utilizamos a função de mostrar os vídeos, verificando visualmente se ele se comporta bem na maioria dos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Margin não faz diferença se o método .detect() for utilizado\n",
    "IMAGE_SIZE = 224\n",
    "MARGIN = 0\n",
    "MIN_FACE_SIZE = 100\n",
    "THRESHOLDS = [0.78, 0.78, 0.78]\n",
    "POST_PROCESS = False\n",
    "SELECT_LARGEST = False\n",
    "KEEP_ALL = True\n",
    "DEVICE = device\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "mtcnn = MTCNN(image_size=IMAGE_SIZE,\n",
    "              margin=MARGIN, \n",
    "              min_face_size=MIN_FACE_SIZE, \n",
    "              thresholds=THRESHOLDS,\n",
    "              post_process=POST_PROCESS,\n",
    "              select_largest=SELECT_LARGEST, \n",
    "              keep_all=KEEP_ALL, \n",
    "              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasta Aleatória e Objeto Videos\n",
    "\n",
    "Definimos inicialmente a coleta de uma pasta aleatória e criamos um objeto Videos para lidar com o acesso aos vídeos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coletamos uma pasta aleatória de folders\n",
    "random_folder = random.choice(folders) + '/'\n",
    "folder_path = './Kaggle Dataset/' + random_folder\n",
    "\n",
    "# Instaciamos uma objeto da classe Videos aplicado ao path obtido\n",
    "videos = Videos(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vídeo Falso\n",
    "\n",
    "Para cancelar a visualização do vídeo em tempo real, apertar a tecla `q` do teclado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a84687b1afb9>:20: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.putText(img=frame, text=label, org=(box[0], box[1]), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=color, thickness=2)\n",
      "<ipython-input-5-a84687b1afb9>:21: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color=[0, 255, 0], thickness=5)\n"
     ]
    }
   ],
   "source": [
    "video_path, video_name, label = videos.getRandomFakeVideo() # Recupera o caminho de um vídeo aleatório na pasta que esteja com a label 'FAKE'\n",
    "\n",
    "# video_path: Caminho para o vídeo\n",
    "# label: string \"REAL\" ou \"FAKE\"\n",
    "# size: se > 0, tornará o rosto um recorte quadrado de dimensões (size, size)\n",
    "# channel: se for passado 'luma', irá mostrar apenas o canal Y (luma) do rosto.\n",
    "showVideo(video_path, label=label, padding=20, size=-1, channel=None, separate_face_box=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vídeo Real\n",
    "\n",
    "Para o vídeo `FAKE` anterior, vamos achar sua versão original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a84687b1afb9>:20: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.putText(img=frame, text=label, org=(box[0], box[1]), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=color, thickness=2)\n",
      "<ipython-input-5-a84687b1afb9>:21: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color=[0, 255, 0], thickness=5)\n"
     ]
    }
   ],
   "source": [
    "video_path_real, video_name_real, label_real = videos.getRealVideo(video_name)\n",
    "\n",
    "# Podemos passar um padding para verificar a quantidade desejada de recorte ao redo do rosto detectado\n",
    "showVideo(video_path_real, label=label_real, padding=20, size=-1, channel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3° Passo: Testar o loop para 1 folder apenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionamos o primeiro folder aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_folder = random.choice(folders) + '/'\n",
    "folder_path = './Kaggle Dataset/' + random_folder\n",
    "\n",
    "# Instaciamos uma objeto da classe Videos aplicado ao path obtido\n",
    "videos = Videos(folder_path)\n",
    "videos_generator = videos.getAllVideosPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos um loop utilizando a função saveCropFaces, que será chamada uma vez por vídeo. Com isso, esperamos e conferimos a pasta para ver se os arquivos estão corretamente lá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Início do folder Faces Dataset Transformer --------------\n",
      "Faces Dataset Transformer: 0.00%...\n",
      "Faces Dataset Transformer: 5.00%...\n",
      "Faces Dataset Transformer: 10.00%...\n",
      "Faces Dataset Transformer: 15.00%...\n",
      "Faces Dataset Transformer: 20.00%...\n",
      "Faces Dataset Transformer: 25.00%...\n",
      "Faces Dataset Transformer: 30.00%...\n",
      "Faces Dataset Transformer: 35.00%...\n",
      "Faces Dataset Transformer: 40.00%...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-0b0c906483e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_video\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVIDEO_DATA\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideos_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     saveCropFaces(*VIDEO_DATA, \n\u001b[0m\u001b[0;32m     25\u001b[0m                   \u001b[0mfolder_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_folder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-53cd4686c99e>\u001b[0m in \u001b[0;36msaveCropFaces\u001b[1;34m(video_path, video_name, label, folder_name, batch_size, padding, size, check_every_frame, channel, folder, size_folder)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mv_len\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# Utiliza o MTCNN para detectar todas as bounding boxes de todos os rostos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Verifica se não foi obtida nenhuma bounding box em todo o batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             batch_boxes, batch_points = detect_face(\n\u001b[0m\u001b[0;32m    314\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_face_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001b[0m in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mimg_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_inds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                 \u001b[0mim_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mim_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mim_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mim_data\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.0078125\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001b[0m in \u001b[0;36mimresample\u001b[1;34m(img, sz)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m     \u001b[0mim_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"area\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[0;32m   3139\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'area'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3140\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'area'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3143\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Machine Learning\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m    934\u001b[0m                 adaptive_avg_pool2d, (input,), input, output_size)\n\u001b[0;32m    935\u001b[0m     \u001b[0m_output_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_list_with_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_output_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 40\n",
    "PADDING = 10\n",
    "SIZE = -1\n",
    "CHECK_EVERY_FRAME = 30\n",
    "CHANNEL = None\n",
    "FOLDER = 'Faces Dataset Transformer'\n",
    "SIZE_FOLDER = ''\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "init = time.time()\n",
    "print(f\"-------------- Início do folder {FOLDER} --------------\")\n",
    "videos_quantity = len(videos.video_files)\n",
    "percentage = 5\n",
    "print_every = int(videos_quantity / (100/percentage))\n",
    "\n",
    "if not os.path.exists(f\"./{FOLDER}/{SIZE_FOLDER}/FAKE\"):\n",
    "    os.mkdir(f\"./{FOLDER}/{SIZE_FOLDER}/FAKE\")\n",
    "    \n",
    "if not os.path.exists(f\"./{FOLDER}/{SIZE_FOLDER}/REAL\"):\n",
    "    os.mkdir(f\"./{FOLDER}/{SIZE_FOLDER}/REAL\")\n",
    "    \n",
    "for n_video, VIDEO_DATA in enumerate(videos_generator):\n",
    "    \n",
    "    saveCropFaces(*VIDEO_DATA, \n",
    "                  folder_name=random_folder[:-1],\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  padding=PADDING, \n",
    "                  size=SIZE, \n",
    "                  check_every_frame=CHECK_EVERY_FRAME, \n",
    "                  channel=CHANNEL, \n",
    "                  folder=FOLDER,\n",
    "                  size_folder=SIZE_FOLDER)\n",
    "    \n",
    "    if n_video % print_every == 0:\n",
    "        print(\"{}: {:.2f}%...\".format(FOLDER, round(n_video / videos_quantity * 100)))\n",
    "        \n",
    "end = time.time()\n",
    "total = end - init\n",
    "print(f\"Tempo que levou para a conclusão de {len(os.listdir(folder_path))} vídeos: {int(total/60) :.0f}:{total % 60:.0f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que demora em torno de 22 minutos para terminar uma pasta. Depende bastante da quantidade de vídeos na pasta. Podemos aumentar o batch_size para 30 para acelerar o processo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4° Passo: Agora, podemos finalmente exportar todo o dataset de vídeos em imagens de rostos. (Pode levar algumas horas para concluir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nossa MTCNN final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Margin não faz diferença se o método .detect() for utilizado\n",
    "IMAGE_SIZE = 224\n",
    "MARGIN = 0\n",
    "MIN_FACE_SIZE = 100\n",
    "THRESHOLDS = [0.78, 0.78, 0.78]\n",
    "POST_PROCESS = False\n",
    "SELECT_LARGEST = False\n",
    "KEEP_ALL = True\n",
    "DEVICE = device\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "mtcnn = MTCNN(image_size=IMAGE_SIZE,\n",
    "              margin=MARGIN, \n",
    "              min_face_size=MIN_FACE_SIZE, \n",
    "              thresholds=THRESHOLDS,\n",
    "              post_process=POST_PROCESS,\n",
    "              select_largest=SELECT_LARGEST, \n",
    "              keep_all=KEEP_ALL, \n",
    "              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui nós removemos os folder que já foram avaliados em momentos anteriores quando rodou-se o script do código. Para novos foldes será necessário escrever manualmente o nome na lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_folders = ['dfdc_train_part_31', \n",
    "                'dfdc_train_part_10', \n",
    "                'dfdc_train_part_40', \n",
    "                'dfdc_train_part_22', \n",
    "                'dfdc_train_part_41', \n",
    "                'dfdc_train_part_21', \n",
    "                'dfdc_train_part_38', \n",
    "                'dfdc_train_part_36', \n",
    "                'dfdc_train_part_43']\n",
    "\n",
    "recent_folders = [folder for folder in folders if folder not in done_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 16, 15, 26,  4, 12, 37, 27, 39,  6, 25,  9, 13, 31, 34,  8, 17,\n",
       "       24,  0, 33,  5, 11,  1, 29, 21,  2, 30, 36,  3, 35, 23, 32, 10, 22,\n",
       "       18, 20,  7, 14, 28, 38])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_order = np.random.RandomState(seed=42).permutation(len(recent_folders))\n",
    "pick_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E iniciamos o loop que pode levar de algumas horas até alguns dias para terminar, dependendo do hardware. Valores importante que devem ser setados nessa etapa por fim:\n",
    "- `CHECK_EVERY_FRAME`: Define de quantos em quantos frames será realizada a inferência da rede para obter o rosto. Caso o valor seja 1 a rede realizará a inferência de 1 em 1 frame, ou seja, tentará encontrar e recortar os rostos de todos os frames de todos os vídeos.\n",
    "- `PADDING`: Controla a margem de recorte das imagens. Quanto maior este número, maior será a margem em volta do rosto recortado.\n",
    "- `SIZE`: Controla as dimensões de saída do rosto recortado. Caso seja `-1`, o rosto será salvo nas dimensões originais obtidas. Como muitos frameworks de Deep Learning contém transformações do tipo **resize** extremamente otimizados, preferi manter o tamanho originalmente obtido permitindo maior flexibilidade com quem venha a implementar a leitura dessas imagens futuramente.\n",
    "- `BATCH_SIZE`: Controla o tamanho do batch de frames que será processado de uma única vez pela MTCNN. Um maior número significa mais paralelado que significa mais rápido, porém pode ser que a memória dos hardwares não suporte valores muito elevados. Exemplo: Minha RTX 2070 com 8GB de memória suporta um máximo batch_size em torno de 30.\n",
    "- `CHANNEL`: Controla o espaço de cores do rosto recortado. Caso seja `None`, será salva a imagem recortada do rosto com os 3 canais originais RGB. A única outra implementação disponível aqui é `'luma'`, que transforma a imagem para o espaço YCrCb e salva somente o canal Y (luma).\n",
    "- `FOLDER`: Nome da pasta onde estará a subpasta que contenha as pastas \"FAKE\" e \"REAL\".\n",
    "- `SIZE_FOLDER`: Nome da subpasta onde estarão as pastas \"FAKE\" e \"REAL\", que a função utilizará para distribuir as imagens corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Início do folder dfdc_train_part_26 --------------\n",
      "dfdc_train_part_29: 0.00%...\n",
      "dfdc_train_part_29: 25.00%...\n",
      "dfdc_train_part_29: 50.00%...\n",
      "dfdc_train_part_29: 75.00%...\n",
      "dfdc_train_part_29: 100.00%...\n",
      "Tempo para a conclusão do diretório: 40:25 min\n",
      "-------------- Início do folder dfdc_train_part_23 --------------\n",
      "dfdc_train_part_26: 0.00%...\n",
      "dfdc_train_part_26: 25.00%...\n",
      "dfdc_train_part_26: 50.00%...\n",
      "dfdc_train_part_26: 75.00%...\n",
      "dfdc_train_part_26: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:15 min\n",
      "-------------- Início do folder dfdc_train_part_22 --------------\n",
      "dfdc_train_part_25: 0.00%...\n",
      "dfdc_train_part_25: 25.00%...\n",
      "dfdc_train_part_25: 50.00%...\n",
      "dfdc_train_part_25: 75.00%...\n",
      "dfdc_train_part_25: 100.00%...\n",
      "Tempo para a conclusão do diretório: 36:0 min\n",
      "-------------- Início do folder dfdc_train_part_32 --------------\n",
      "dfdc_train_part_37: 0.00%...\n",
      "dfdc_train_part_37: 25.00%...\n",
      "dfdc_train_part_37: 50.00%...\n",
      "dfdc_train_part_37: 75.00%...\n",
      "dfdc_train_part_37: 100.00%...\n",
      "Tempo para a conclusão do diretório: 36:56 min\n",
      "-------------- Início do folder dfdc_train_part_12 --------------\n",
      "dfdc_train_part_13: 0.00%...\n",
      "dfdc_train_part_13: 25.00%...\n",
      "dfdc_train_part_13: 50.00%...\n",
      "dfdc_train_part_13: 75.00%...\n",
      "dfdc_train_part_13: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:27 min\n",
      "-------------- Início do folder dfdc_train_part_2 --------------\n",
      "dfdc_train_part_20: 0.00%...\n",
      "dfdc_train_part_20: 25.00%...\n",
      "dfdc_train_part_20: 50.00%...\n",
      "dfdc_train_part_20: 75.00%...\n",
      "dfdc_train_part_20: 100.00%...\n",
      "Tempo para a conclusão do diretório: 32:21 min\n",
      "-------------- Início do folder dfdc_train_part_42 --------------\n",
      "dfdc_train_part_7: 0.00%...\n",
      "dfdc_train_part_7: 25.00%...\n",
      "dfdc_train_part_7: 50.00%...\n",
      "dfdc_train_part_7: 75.00%...\n",
      "dfdc_train_part_7: 100.00%...\n",
      "Tempo para a conclusão do diretório: 37:58 min\n",
      "-------------- Início do folder dfdc_train_part_33 --------------\n",
      "dfdc_train_part_39: 0.00%...\n",
      "dfdc_train_part_39: 25.00%...\n",
      "dfdc_train_part_39: 50.00%...\n",
      "dfdc_train_part_39: 75.00%...\n",
      "Tempo para a conclusão do diretório: 38:14 min\n",
      "-------------- Início do folder dfdc_train_part_44 --------------\n",
      "dfdc_train_part_9: 0.00%...\n",
      "dfdc_train_part_9: 25.00%...\n",
      "dfdc_train_part_9: 50.00%...\n",
      "dfdc_train_part_9: 75.00%...\n",
      "Tempo para a conclusão do diretório: 26:36 min\n",
      "-------------- Início do folder dfdc_train_part_14 --------------\n",
      "dfdc_train_part_15: 0.00%...\n",
      "dfdc_train_part_15: 25.00%...\n",
      "dfdc_train_part_15: 50.00%...\n",
      "dfdc_train_part_15: 75.00%...\n",
      "dfdc_train_part_15: 100.00%...\n",
      "Tempo para a conclusão do diretório: 32:24 min\n",
      "-------------- Início do folder dfdc_train_part_31 --------------\n",
      "dfdc_train_part_35: 0.00%...\n",
      "dfdc_train_part_35: 25.00%...\n",
      "dfdc_train_part_35: 50.00%...\n",
      "dfdc_train_part_35: 75.00%...\n",
      "dfdc_train_part_35: 100.00%...\n",
      "Tempo para a conclusão do diretório: 36:28 min\n",
      "-------------- Início do folder dfdc_train_part_17 --------------\n",
      "dfdc_train_part_18: 0.00%...\n",
      "dfdc_train_part_18: 25.00%...\n",
      "dfdc_train_part_18: 50.00%...\n",
      "dfdc_train_part_18: 75.00%...\n",
      "dfdc_train_part_18: 100.00%...\n",
      "Tempo para a conclusão do diretório: 37:37 min\n",
      "-------------- Início do folder dfdc_train_part_20 --------------\n",
      "dfdc_train_part_23: 0.00%...\n",
      "dfdc_train_part_23: 25.00%...\n",
      "dfdc_train_part_23: 50.00%...\n",
      "dfdc_train_part_23: 75.00%...\n",
      "dfdc_train_part_23: 100.00%...\n",
      "Tempo para a conclusão do diretório: 36:8 min\n",
      "-------------- Início do folder dfdc_train_part_37 --------------\n",
      "dfdc_train_part_45: 0.00%...\n",
      "dfdc_train_part_45: 25.00%...\n",
      "dfdc_train_part_45: 50.00%...\n",
      "dfdc_train_part_45: 75.00%...\n",
      "dfdc_train_part_45: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:36 min\n",
      "-------------- Início do folder dfdc_train_part_4 --------------\n",
      "dfdc_train_part_48: 0.00%...\n",
      "dfdc_train_part_48: 25.00%...\n",
      "dfdc_train_part_48: 50.00%...\n",
      "dfdc_train_part_48: 75.00%...\n",
      "dfdc_train_part_48: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:47 min\n",
      "-------------- Início do folder dfdc_train_part_16 --------------\n",
      "dfdc_train_part_17: 0.00%...\n",
      "dfdc_train_part_17: 25.00%...\n",
      "dfdc_train_part_17: 50.00%...\n",
      "dfdc_train_part_17: 75.00%...\n",
      "dfdc_train_part_17: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:36 min\n",
      "-------------- Início do folder dfdc_train_part_24 --------------\n",
      "dfdc_train_part_27: 0.00%...\n",
      "dfdc_train_part_27: 25.00%...\n",
      "dfdc_train_part_27: 50.00%...\n",
      "dfdc_train_part_27: 75.00%...\n",
      "dfdc_train_part_27: 100.00%...\n",
      "Tempo para a conclusão do diretório: 31:25 min\n",
      "-------------- Início do folder dfdc_train_part_30 --------------\n",
      "dfdc_train_part_34: 0.00%...\n",
      "dfdc_train_part_34: 25.00%...\n",
      "dfdc_train_part_34: 50.00%...\n",
      "dfdc_train_part_34: 75.00%...\n",
      "dfdc_train_part_34: 100.00%...\n",
      "Tempo para a conclusão do diretório: 39:52 min\n",
      "-------------- Início do folder dfdc_train_part_0 --------------\n",
      "dfdc_train_part_0: 0.00%...\n",
      "dfdc_train_part_0: 25.00%...\n",
      "dfdc_train_part_0: 50.00%...\n",
      "dfdc_train_part_0: 75.00%...\n",
      "dfdc_train_part_0: 100.00%...\n",
      "Tempo para a conclusão do diretório: 21:9 min\n",
      "-------------- Início do folder dfdc_train_part_39 --------------\n",
      "dfdc_train_part_47: 0.00%...\n",
      "dfdc_train_part_47: 25.00%...\n",
      "dfdc_train_part_47: 50.00%...\n",
      "dfdc_train_part_47: 75.00%...\n",
      "dfdc_train_part_47: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:56 min\n",
      "-------------- Início do folder dfdc_train_part_13 --------------\n",
      "dfdc_train_part_14: 0.00%...\n",
      "dfdc_train_part_14: 25.00%...\n",
      "dfdc_train_part_14: 50.00%...\n",
      "dfdc_train_part_14: 75.00%...\n",
      "Tempo para a conclusão do diretório: 38:8 min\n",
      "-------------- Início do folder dfdc_train_part_19 --------------\n",
      "dfdc_train_part_2: 0.00%...\n",
      "dfdc_train_part_2: 25.00%...\n",
      "dfdc_train_part_2: 50.00%...\n",
      "dfdc_train_part_2: 75.00%...\n",
      "Tempo para a conclusão do diretório: 28:51 min\n",
      "-------------- Início do folder dfdc_train_part_1 --------------\n",
      "dfdc_train_part_1: 0.00%...\n",
      "dfdc_train_part_1: 25.00%...\n",
      "dfdc_train_part_1: 50.00%...\n",
      "dfdc_train_part_1: 75.00%...\n",
      "dfdc_train_part_1: 100.00%...\n",
      "Tempo para a conclusão do diretório: 26:38 min\n",
      "-------------- Início do folder dfdc_train_part_35 --------------\n",
      "dfdc_train_part_42: 0.00%...\n",
      "dfdc_train_part_42: 25.00%...\n",
      "dfdc_train_part_42: 50.00%...\n",
      "dfdc_train_part_42: 75.00%...\n",
      "Tempo para a conclusão do diretório: 37:32 min\n",
      "-------------- Início do folder dfdc_train_part_28 --------------\n",
      "dfdc_train_part_30: 0.00%...\n",
      "dfdc_train_part_30: 25.00%...\n",
      "dfdc_train_part_30: 50.00%...\n",
      "dfdc_train_part_30: 75.00%...\n",
      "Tempo para a conclusão do diretório: 34:43 min\n",
      "-------------- Início do folder dfdc_train_part_10 --------------\n",
      "dfdc_train_part_11: 0.00%...\n",
      "dfdc_train_part_11: 25.00%...\n",
      "dfdc_train_part_11: 50.00%...\n",
      "dfdc_train_part_11: 75.00%...\n",
      "dfdc_train_part_11: 100.00%...\n",
      "Tempo para a conclusão do diretório: 31:10 min\n",
      "-------------- Início do folder dfdc_train_part_36 --------------\n",
      "dfdc_train_part_44: 0.00%...\n",
      "dfdc_train_part_44: 25.00%...\n",
      "dfdc_train_part_44: 50.00%...\n",
      "dfdc_train_part_44: 75.00%...\n",
      "dfdc_train_part_44: 100.00%...\n",
      "Tempo para a conclusão do diretório: 39:56 min\n",
      "-------------- Início do folder dfdc_train_part_41 --------------\n",
      "dfdc_train_part_6: 0.00%...\n",
      "dfdc_train_part_6: 25.00%...\n",
      "dfdc_train_part_6: 50.00%...\n",
      "dfdc_train_part_6: 75.00%...\n",
      "Tempo para a conclusão do diretório: 25:25 min\n",
      "-------------- Início do folder dfdc_train_part_11 --------------\n",
      "dfdc_train_part_12: 0.00%...\n",
      "dfdc_train_part_12: 25.00%...\n",
      "dfdc_train_part_12: 50.00%...\n",
      "dfdc_train_part_12: 75.00%...\n",
      "dfdc_train_part_12: 100.00%...\n",
      "Tempo para a conclusão do diretório: 35:28 min\n",
      "-------------- Início do folder dfdc_train_part_40 --------------\n",
      "dfdc_train_part_5: 0.00%...\n",
      "dfdc_train_part_5: 25.00%...\n",
      "dfdc_train_part_5: 50.00%...\n",
      "dfdc_train_part_5: 75.00%...\n",
      "dfdc_train_part_5: 100.00%...\n",
      "Tempo para a conclusão do diretório: 41:22 min\n",
      "-------------- Início do folder dfdc_train_part_3 --------------\n",
      "dfdc_train_part_33: 0.00%...\n",
      "dfdc_train_part_33: 25.00%...\n",
      "dfdc_train_part_33: 50.00%...\n",
      "dfdc_train_part_33: 75.00%...\n",
      "dfdc_train_part_33: 100.00%...\n",
      "Tempo para a conclusão do diretório: 34:16 min\n",
      "-------------- Início do folder dfdc_train_part_38 --------------\n",
      "dfdc_train_part_46: 0.00%...\n",
      "dfdc_train_part_46: 25.00%...\n",
      "dfdc_train_part_46: 50.00%...\n",
      "dfdc_train_part_46: 75.00%...\n",
      "dfdc_train_part_46: 100.00%...\n",
      "Tempo para a conclusão do diretório: 32:13 min\n",
      "-------------- Início do folder dfdc_train_part_18 --------------\n",
      "dfdc_train_part_19: 0.00%...\n",
      "dfdc_train_part_19: 25.00%...\n",
      "dfdc_train_part_19: 50.00%...\n",
      "dfdc_train_part_19: 75.00%...\n",
      "dfdc_train_part_19: 100.00%...\n",
      "dfdc_train_part_19: 125.00%...\n",
      "dfdc_train_part_19: 150.00%...\n",
      "Tempo para a conclusão do diretório: 23:13 min\n",
      "-------------- Início do folder dfdc_train_part_29 --------------\n",
      "dfdc_train_part_32: 0.00%...\n",
      "dfdc_train_part_32: 25.00%...\n",
      "dfdc_train_part_32: 50.00%...\n",
      "dfdc_train_part_32: 75.00%...\n",
      "Tempo para a conclusão do diretório: 37:44 min\n",
      "-------------- Início do folder dfdc_train_part_25 --------------\n",
      "dfdc_train_part_28: 0.00%...\n",
      "dfdc_train_part_28: 25.00%...\n",
      "dfdc_train_part_28: 50.00%...\n",
      "dfdc_train_part_28: 75.00%...\n",
      "dfdc_train_part_28: 100.00%...\n",
      "Tempo para a conclusão do diretório: 32:10 min\n",
      "-------------- Início do folder dfdc_train_part_27 --------------\n",
      "dfdc_train_part_3: 0.00%...\n",
      "dfdc_train_part_3: 25.00%...\n",
      "dfdc_train_part_3: 50.00%...\n",
      "dfdc_train_part_3: 75.00%...\n",
      "dfdc_train_part_3: 100.00%...\n",
      "Tempo para a conclusão do diretório: 21:49 min\n",
      "-------------- Início do folder dfdc_train_part_15 --------------\n",
      "dfdc_train_part_16: 0.00%...\n",
      "dfdc_train_part_16: 25.00%...\n",
      "dfdc_train_part_16: 50.00%...\n",
      "dfdc_train_part_16: 75.00%...\n",
      "dfdc_train_part_16: 100.00%...\n",
      "Tempo para a conclusão do diretório: 32:55 min\n",
      "-------------- Início do folder dfdc_train_part_21 --------------\n",
      "dfdc_train_part_24: 0.00%...\n",
      "dfdc_train_part_24: 25.00%...\n",
      "dfdc_train_part_24: 50.00%...\n",
      "dfdc_train_part_24: 75.00%...\n",
      "dfdc_train_part_24: 100.00%...\n",
      "Tempo para a conclusão do diretório: 71:40 min\n",
      "-------------- Início do folder dfdc_train_part_34 --------------\n",
      "dfdc_train_part_4: 0.00%...\n",
      "dfdc_train_part_4: 25.00%...\n",
      "dfdc_train_part_4: 50.00%...\n",
      "dfdc_train_part_4: 75.00%...\n",
      "dfdc_train_part_4: 100.00%...\n",
      "Tempo para a conclusão do diretório: 30:40 min\n",
      "-------------- Início do folder dfdc_train_part_43 --------------\n",
      "dfdc_train_part_8: 0.00%...\n",
      "dfdc_train_part_8: 25.00%...\n",
      "dfdc_train_part_8: 50.00%...\n",
      "dfdc_train_part_8: 75.00%...\n",
      "Tempo para a conclusão do diretório: 30:30 min\n"
     ]
    }
   ],
   "source": [
    "PATH = './Kaggle Dataset/'\n",
    "CHECK_EVERY_FRAME = 15\n",
    "PADDING = 10\n",
    "SIZE = -1\n",
    "BATCH_SIZE = 20\n",
    "CHANNEL = None\n",
    "FOLDER = 'Faces Dataset'\n",
    "SIZE_FOLDER = 'no-resize-color'\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "for pick in pick_order:\n",
    "    folder_path = PATH + recent_folders[pick]\n",
    "    # Instaciamos uma objeto da classe Videos aplicado ao path obtido\n",
    "    videos = Videos(folder_path)\n",
    "    videos_generator = videos.getAllVideosPath()    \n",
    "    \n",
    "    print(\"-------------- Início do folder {} --------------\".format(folders[pick]))\n",
    "    videos_quantity = len(videos.video_files)\n",
    "    percentage = 25\n",
    "    print_every = int(videos_quantity / (100/percentage))\n",
    "    \n",
    "    init = time.time()\n",
    "    for n_video, VIDEO_DATA in enumerate(videos_generator):\n",
    "        \n",
    "        saveCropFaces(*VIDEO_DATA, \n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      padding=PADDING, \n",
    "                      size=SIZE, \n",
    "                      check_every_frame=CHECK_EVERY_FRAME, \n",
    "                      channel=CHANNEL, \n",
    "                      folder=FOLDER,\n",
    "                      size_folder=SIZE_FOLDER)\n",
    "        \n",
    "        if n_video % print_every == 0:\n",
    "            print(\"{}: {:.2f}%...\".format(recent_folders[pick], round(n_video / videos_quantity * 100)))\n",
    "        \n",
    "        \n",
    "    end = time.time()\n",
    "    total = end - init\n",
    "    print(\"Tempo para a conclusão do diretório: {:.0f}:{:.0f} min\".format(int(total/60), total % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui o processo foi interrompido pois a quantidade total de imagens de rostos recortadas já superava o meio milhão, suficiente para rodar algumas redes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
