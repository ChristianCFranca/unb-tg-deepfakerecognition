{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import io\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega uma rede ResNet18 pr√© treinada no ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "DeepFakeDataset = torchvision.datasets.ImageFolder('./Faces Dataset/128px/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "percentage_for_train = 0.7\n",
    "\n",
    "shuffle_indices = torch.randperm(len(DeepFakeDataset))\n",
    "train_indices = shuffle_indices[:int(percentage_for_train*len(DeepFakeDataset))]\n",
    "val_indices = shuffle_indices[int(percentage_for_train*len(DeepFakeDataset)):]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(DeepFakeDataset, batch_size=126, sampler=train_sampler)\n",
    "test_dataloader = torch.utils.data.DataLoader(DeepFakeDataset, batch_size=126, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def training_loop(n_epochs, model, loss_function, optimizer):\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss_sum = 0\n",
    "        iteration = 0\n",
    "        print(\"Beggining epoch {}...\".format(epoch+1))\n",
    "        for images, labels in train_dataloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss = loss_function(predictions, labels.long())\n",
    "            loss_sum += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 20 == 0:\n",
    "                print(\"Iteration {} Loss {:.5f}\".format(iteration, loss_sum.item()/20))\n",
    "                loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialmente, rodaremos o modelo cru no dataset para checar seu desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model):\n",
    "    all_labels = torch.LongTensor([]).cuda()\n",
    "    all_predictions = torch.LongTensor([]).cuda()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(images)\n",
    "            predictions = predictions.max(dim=1)[1]\n",
    "            all_predictions = torch.cat((all_predictions, predictions))\n",
    "            all_labels = torch.cat((all_labels, labels))\n",
    "            \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame_conf_matrix(all_pread, all_labels):\n",
    "    c_mat = pd.DataFrame(metrics.confusion_matrix(all_labels.cpu().detach().numpy(), all_pred.cpu().detach().numpy()))\n",
    "    c_mat = c_mat.rename(columns={0: 'FAKE_predicted', 1: \"REAL_predicted\"}, index={0: 'FAKE', 1: \"REAL\"})\n",
    "    rows = [c_mat.loc['FAKE'][0] / c_mat.sum()[0] * 100, c_mat.loc['REAL'][1] / c_mat.sum()[1] * 100]\n",
    "    columns = [c_mat['FAKE_predicted'][0] / c_mat.sum(axis=1)[0] * 100, c_mat['REAL_predicted'][1] / c_mat.sum(axis=1)[1] * 100]\n",
    "    accuracy = (c_mat.loc['FAKE'][0] + c_mat.loc['REAL'][1]) / (c_mat.sum(axis=1)[0] + c_mat.sum(axis=1)[1])*100 \n",
    "    c_mat.loc['Percentage'] = [str(round(rows[0],2)) +' %', str(round(rows[1],2)) +' %']\n",
    "    c_mat['Percentage'] = [str(round(columns[0],2)) +' %', str(round(columns[1],2)) +' %', str(round(accuracy,2)) +' %']\n",
    "    return c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = all_pred.to(dtype=torch.int8)\n",
    "all_labels = all_labels.to(dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>10386</td>\n",
       "      <td>6253</td>\n",
       "      <td>62.42 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>2150</td>\n",
       "      <td>839</td>\n",
       "      <td>28.07 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>82.85 %</td>\n",
       "      <td>11.83 %</td>\n",
       "      <td>57.19 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                10386           6253    62.42 %\n",
       "REAL                 2150            839    28.07 %\n",
       "Percentage        82.85 %        11.83 %    57.19 %"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temos um total de 10386 VERDADEIROS POSITIVOS. \n",
    "- Temos um total de 6253 FALSOS NEGATIVOS.\n",
    "- Temos um total de 2150 FALSOS POSITIVOS.\n",
    "- Temos um total de 839 VERDADEIROS NEGATIVOS.\n",
    "\n",
    "Com isso, temos as seguintes m√©tricas:\n",
    "- Acur√°cia: 57.19% = (TP+TN)/Total\n",
    "\n",
    "- Precis√£o: 82.85% = TP/P_predicted\n",
    "- False Omission Rate (FOR): 11.83% = FN/N_predicted\n",
    "- Recall/Sensibilidade/Probabilidade de detec√ß√£o: 62.42% = TP/P\n",
    "- Especificidade: 28.07% = TN/N\n",
    "- Probabilidade de Alarme Falso = 1 - Especificidade = 71.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora, treinamos o modelo e testamos novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 0...\n",
      "Iteration 20 Loss 0.44600\n",
      "Iteration 40 Loss 0.31668\n",
      "Iteration 60 Loss 0.32156\n",
      "Iteration 80 Loss 0.36242\n",
      "Iteration 100 Loss 0.33671\n",
      "Iteration 120 Loss 0.30961\n",
      "Iteration 140 Loss 0.40002\n",
      "Iteration 160 Loss 0.28032\n",
      "Iteration 180 Loss 0.29681\n",
      "Iteration 200 Loss 0.31612\n",
      "Iteration 220 Loss 0.29692\n",
      "Iteration 240 Loss 0.25674\n",
      "Iteration 260 Loss 0.34980\n",
      "Iteration 280 Loss 0.32549\n",
      "Iteration 300 Loss 0.30501\n",
      "Iteration 320 Loss 0.25702\n",
      "Iteration 340 Loss 0.33320\n",
      "Iteration 360 Loss 0.37822\n",
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.27416\n",
      "Iteration 40 Loss 0.17406\n",
      "Iteration 60 Loss 0.36319\n",
      "Iteration 80 Loss 0.22675\n",
      "Iteration 100 Loss 0.30393\n",
      "Iteration 120 Loss 0.32933\n",
      "Iteration 140 Loss 0.22070\n",
      "Iteration 160 Loss 0.27466\n",
      "Iteration 180 Loss 0.30978\n",
      "Iteration 200 Loss 0.31370\n",
      "Iteration 220 Loss 0.21768\n",
      "Iteration 240 Loss 0.26396\n",
      "Iteration 260 Loss 0.23313\n",
      "Iteration 280 Loss 0.29257\n",
      "Iteration 300 Loss 0.30907\n",
      "Iteration 320 Loss 0.31404\n",
      "Iteration 340 Loss 0.38593\n",
      "Iteration 360 Loss 0.30588\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>16464</td>\n",
       "      <td>175</td>\n",
       "      <td>98.95 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>2079</td>\n",
       "      <td>910</td>\n",
       "      <td>30.44 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>88.79 %</td>\n",
       "      <td>83.87 %</td>\n",
       "      <td>88.52 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                16464            175    98.95 %\n",
       "REAL                 2079            910    30.44 %\n",
       "Percentage        88.79 %        83.87 %    88.52 %"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi obtida uma acur√°cia de 88.52% dessa vez, bem melhor! Resolvemos o problema do DeepFake ent√£o, certo? Bem... n√£o.\n",
    "O dataset possui aproximadamente 40 mil imagens (93% do dataset) a mais de DeepFakes do que rostos verdadeiros.\n",
    "\n",
    "Aqui a nossa medida est√° sendo inteiramente a acur√°cia. Mas se observamos os resultados, vemos que dos 2989 rostos que n√£o s√£o DeepFakes, o modelo disse que 2079 deles s√£o.\n",
    "O que est√° acontecendo aqui √© que nosso dataset cont√©m aproximadamente 40 mil rostos a mais de DeepFakes do que rostos reais, em uma propor√ß√£o de aproximadamente 6:1. Dessa forma, o modelo encontrou um jeito f√°cil de ter uma alta acur√°cia sem necessariamente aprender a diferenciar um DeepFake de um rosto real: basta chutar que a maioria dos rostos s√£o DeepFakes!\n",
    "Com essa t√°tica, o modelo classificou corretamente 98.95% dos DeepFakes, mas se observarmos os rostos verdadeiros, de um total de 2989 rostos, o modelo classificou 2079 como DeepFakes, acertando apenas 30.44% dos rostos reais. Se voc√™ acreditar no modelo sempre que ele falar que determinada imagem √© um DeepFake, voc√™ praticamente nunca deixar√° um DeepFake passar sem ser detectado. Excelente!... a n√£o ser pelo fato de que ser t√£o diligente assim significa que voc√™ n√£o est√° realmente poupando algum trabalho por usar uma rede neural para realizar esse servi√ßo, uma vez que os v√≠deos que s√£o de rostos reais tamb√©m ser√£o classificados como DeepFakes e ser√° necess√°rio inspecion√°-los de qualquer jeito, caso contr√°rio as consequ√™ncias podem ser graves.\n",
    "\n",
    "\n",
    "Nosso modelo tem uma alta taxa de Falsos Positivos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que podemos fazer para tentar ajustar isso? No crit√©rio de avalia√ß√£o, no caso, a nossa fun√ß√£o de custo CrossEntropyLoss, existe a possibilidade de ponderar o peso que cada classe tem caso seja julgado corretamente ou incorretamente. Para isso, passamos uma lista com o peso de cada classe para a fun√ß√£o de custo. Recarregamos o modelo e treinamos novamente com as devidas altera√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.66378\n",
      "Iteration 40 Loss 0.57324\n",
      "Iteration 60 Loss 0.51859\n",
      "Iteration 80 Loss 0.56014\n",
      "Iteration 100 Loss 0.52884\n",
      "Iteration 120 Loss 0.60112\n",
      "Iteration 140 Loss 0.53312\n",
      "Iteration 160 Loss 0.54665\n",
      "Iteration 180 Loss 0.50956\n",
      "Iteration 200 Loss 0.67105\n",
      "Iteration 220 Loss 0.47249\n",
      "Iteration 240 Loss 0.43156\n",
      "Iteration 260 Loss 0.35011\n",
      "Iteration 280 Loss 0.42339\n",
      "Iteration 300 Loss 0.49906\n",
      "Iteration 320 Loss 0.37456\n",
      "Iteration 340 Loss 0.51171\n",
      "Iteration 360 Loss 0.39230\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.32898\n",
      "Iteration 40 Loss 0.48226\n",
      "Iteration 60 Loss 0.36133\n",
      "Iteration 80 Loss 0.38823\n",
      "Iteration 100 Loss 0.39688\n",
      "Iteration 120 Loss 0.46556\n",
      "Iteration 140 Loss 0.47131\n",
      "Iteration 160 Loss 0.38778\n",
      "Iteration 180 Loss 0.54917\n",
      "Iteration 200 Loss 0.39032\n",
      "Iteration 220 Loss 0.50269\n",
      "Iteration 240 Loss 0.47832\n",
      "Iteration 260 Loss 0.39394\n",
      "Iteration 280 Loss 0.56876\n",
      "Iteration 300 Loss 0.31022\n",
      "Iteration 320 Loss 0.46411\n",
      "Iteration 340 Loss 0.45022\n",
      "Iteration 360 Loss 0.41879\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 3x maior para a classe de n√£o deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 3.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>14297</td>\n",
       "      <td>2342</td>\n",
       "      <td>85.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>991</td>\n",
       "      <td>1998</td>\n",
       "      <td>66.85 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>93.52 %</td>\n",
       "      <td>46.04 %</td>\n",
       "      <td>83.02 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                14297           2342    85.92 %\n",
       "REAL                  991           1998    66.85 %\n",
       "Percentage        93.52 %        46.04 %    83.02 %"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos melhorar sutilmente nosso resultado para a classe REAL. Podemos explorar um peso ainda maior agora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.66142\n",
      "Iteration 40 Loss 0.63493\n",
      "Iteration 60 Loss 0.57894\n",
      "Iteration 80 Loss 0.62384\n",
      "Iteration 100 Loss 0.53665\n",
      "Iteration 120 Loss 0.54088\n",
      "Iteration 140 Loss 0.53001\n",
      "Iteration 160 Loss 0.47315\n",
      "Iteration 180 Loss 0.56780\n",
      "Iteration 200 Loss 0.56354\n",
      "Iteration 220 Loss 0.51406\n",
      "Iteration 240 Loss 0.49521\n",
      "Iteration 260 Loss 0.54355\n",
      "Iteration 280 Loss 0.54263\n",
      "Iteration 300 Loss 0.48048\n",
      "Iteration 320 Loss 0.58414\n",
      "Iteration 340 Loss 0.52512\n",
      "Iteration 360 Loss 0.53464\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.49739\n",
      "Iteration 40 Loss 0.42896\n",
      "Iteration 60 Loss 0.54676\n",
      "Iteration 80 Loss 0.39877\n",
      "Iteration 100 Loss 0.54500\n",
      "Iteration 120 Loss 0.49910\n",
      "Iteration 140 Loss 0.53410\n",
      "Iteration 160 Loss 0.43038\n",
      "Iteration 180 Loss 0.43762\n",
      "Iteration 200 Loss 0.42918\n",
      "Iteration 220 Loss 0.46585\n",
      "Iteration 240 Loss 0.37796\n",
      "Iteration 260 Loss 0.48875\n",
      "Iteration 280 Loss 0.47058\n",
      "Iteration 300 Loss 0.53969\n",
      "Iteration 320 Loss 0.53361\n",
      "Iteration 340 Loss 0.49276\n",
      "Iteration 360 Loss 0.45491\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 8x maior para a classe de n√£o deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 8.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>12105</td>\n",
       "      <td>4534</td>\n",
       "      <td>72.75 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>571</td>\n",
       "      <td>2418</td>\n",
       "      <td>80.9 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>95.5 %</td>\n",
       "      <td>34.78 %</td>\n",
       "      <td>73.99 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                12105           4534    72.75 %\n",
       "REAL                  571           2418     80.9 %\n",
       "Percentage         95.5 %        34.78 %    73.99 %"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem melhor! Infelizmente nossa acur√°cia geral caiu, mas o modelo conseguiu predizer mais rostos reais como reais e mais rostos falsos como falsos. Ainda h√° um longo caminho a se percorrer. Por hora, vamos tentar diminuir um pouco o learning rate e aumentar o n√∫mero de √©pocas utilizando um modelo mais poderoso e observar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.74290\n",
      "Iteration 40 Loss 0.69050\n",
      "Iteration 60 Loss 0.64445\n",
      "Iteration 80 Loss 0.63979\n",
      "Iteration 100 Loss 0.59284\n",
      "Iteration 120 Loss 0.59329\n",
      "Iteration 140 Loss 0.58190\n",
      "Iteration 160 Loss 0.55730\n",
      "Iteration 180 Loss 0.57475\n",
      "Iteration 200 Loss 0.58001\n",
      "Iteration 220 Loss 0.53973\n",
      "Iteration 240 Loss 0.56590\n",
      "Iteration 260 Loss 0.55069\n",
      "Iteration 280 Loss 0.53715\n",
      "Iteration 300 Loss 0.53509\n",
      "Iteration 320 Loss 0.53396\n",
      "Iteration 340 Loss 0.54495\n",
      "Iteration 360 Loss 0.54519\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.51861\n",
      "Iteration 40 Loss 0.54998\n",
      "Iteration 60 Loss 0.50373\n",
      "Iteration 80 Loss 0.51190\n",
      "Iteration 100 Loss 0.50706\n",
      "Iteration 120 Loss 0.52625\n",
      "Iteration 140 Loss 0.50254\n",
      "Iteration 160 Loss 0.52229\n",
      "Iteration 180 Loss 0.49917\n",
      "Iteration 200 Loss 0.51160\n",
      "Iteration 220 Loss 0.50501\n",
      "Iteration 240 Loss 0.50059\n",
      "Iteration 260 Loss 0.50135\n",
      "Iteration 280 Loss 0.46850\n",
      "Iteration 300 Loss 0.47726\n",
      "Iteration 320 Loss 0.51384\n",
      "Iteration 340 Loss 0.49972\n",
      "Iteration 360 Loss 0.51220\n",
      "Beggining epoch 3...\n",
      "Iteration 20 Loss 0.47358\n",
      "Iteration 40 Loss 0.48821\n",
      "Iteration 60 Loss 0.49637\n",
      "Iteration 80 Loss 0.49251\n",
      "Iteration 100 Loss 0.49014\n",
      "Iteration 120 Loss 0.49817\n",
      "Iteration 140 Loss 0.48201\n",
      "Iteration 160 Loss 0.47955\n",
      "Iteration 180 Loss 0.48280\n",
      "Iteration 200 Loss 0.46180\n",
      "Iteration 220 Loss 0.49370\n",
      "Iteration 240 Loss 0.47910\n",
      "Iteration 260 Loss 0.49834\n",
      "Iteration 280 Loss 0.47183\n",
      "Iteration 300 Loss 0.47228\n",
      "Iteration 320 Loss 0.51562\n",
      "Iteration 340 Loss 0.48200\n",
      "Iteration 360 Loss 0.46087\n"
     ]
    }
   ],
   "source": [
    "resnet34 = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet34.fc.in_features\n",
    "\n",
    "resnet34.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 9x maior para a classe de n√£o deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 7.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet34.fc.parameters(), lr=0.4e-3)\n",
    "\n",
    "training_loop(n_epochs=3, model=resnet34, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>12394</td>\n",
       "      <td>4245</td>\n",
       "      <td>74.49 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>604</td>\n",
       "      <td>2385</td>\n",
       "      <td>79.79 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>95.35 %</td>\n",
       "      <td>35.97 %</td>\n",
       "      <td>75.3 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                12394           4245    74.49 %\n",
       "REAL                  604           2385    79.79 %\n",
       "Percentage        95.35 %        35.97 %     75.3 %"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet34)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um resultado promissor. Embora nossa acur√°cia geral tenha aumentado apenas um pouco para 75.3%, fomos capazes de classificar corretamente 74.49% dos deepfakes e 79.79% dos reais. Estamos indo certo em alguma dire√ß√£o!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet34.state_dict(), './Saved Models/resnet34_pretrained_trainedover2folders.pt')\n",
    "\n",
    "#torch.save({\n",
    "#            'epoch': epoch,\n",
    "#            'model_state_dict': model.state_dict(),\n",
    "#            'optimizer_state_dict': optimizer.state_dict(),\n",
    "#            'loss': loss,\n",
    "#            ...\n",
    "#            }, PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
