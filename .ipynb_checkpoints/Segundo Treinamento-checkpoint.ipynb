{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import io\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega uma rede ResNet18 pré treinada no ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "DeepFakeDataset = torchvision.datasets.ImageFolder('./Faces Dataset/128px/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "percentage_for_train = 0.7\n",
    "\n",
    "shuffle_indices = torch.randperm(len(DeepFakeDataset))\n",
    "train_indices = shuffle_indices[:int(percentage_for_train*len(DeepFakeDataset))]\n",
    "val_indices = shuffle_indices[int(percentage_for_train*len(DeepFakeDataset)):]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(DeepFakeDataset, batch_size=126, sampler=train_sampler)\n",
    "test_dataloader = torch.utils.data.DataLoader(DeepFakeDataset, batch_size=126, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def training_loop(n_epochs, model, loss_function, optimizer):\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(0, n_epochs):\n",
    "        loss_sum = 0\n",
    "        iteration = 0\n",
    "        print(\"Beggining epoch {}...\".format(epoch+1))\n",
    "        for images, labels in train_dataloader:\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss = loss_function(predictions, labels.long())\n",
    "            loss_sum += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 20 == 0:\n",
    "                print(\"Iteration {} Loss {:.5f}\".format(iteration, loss_sum.item()/20))\n",
    "                loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialmente, rodaremos o modelo cru no dataset para checar seu desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model):\n",
    "    all_labels = torch.LongTensor([]).cuda()\n",
    "    all_predictions = torch.LongTensor([]).cuda()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(images)\n",
    "            predictions = predictions.max(dim=1)[1]\n",
    "            all_predictions = torch.cat((all_predictions, predictions))\n",
    "            all_labels = torch.cat((all_labels, labels))\n",
    "            \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame_conf_matrix(all_pread, all_labels):\n",
    "    c_mat = pd.DataFrame(metrics.confusion_matrix(all_labels.cpu().detach().numpy(), all_pred.cpu().detach().numpy()))\n",
    "    c_mat = c_mat.rename(columns={0: 'FAKE_predicted', 1: \"REAL_predicted\"}, index={0: 'FAKE', 1: \"REAL\"})\n",
    "    rows = [c_mat.loc['FAKE'][0] / c_mat.sum()[0] * 100, c_mat.loc['REAL'][1] / c_mat.sum()[1] * 100]\n",
    "    columns = [c_mat['FAKE_predicted'][0] / c_mat.sum(axis=1)[0] * 100, c_mat['REAL_predicted'][1] / c_mat.sum(axis=1)[1] * 100]\n",
    "    accuracy = (c_mat.loc['FAKE'][0] + c_mat.loc['REAL'][1]) / (c_mat.sum(axis=1)[0] + c_mat.sum(axis=1)[1])*100 \n",
    "    c_mat.loc['Percentage'] = [str(round(rows[0],2)) +' %', str(round(rows[1],2)) +' %']\n",
    "    c_mat['Percentage'] = [str(round(columns[0],2)) +' %', str(round(columns[1],2)) +' %', str(round(accuracy,2)) +' %']\n",
    "    return c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = all_pred.to(dtype=torch.int8)\n",
    "all_labels = all_labels.to(dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>10386</td>\n",
       "      <td>6253</td>\n",
       "      <td>62.42 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>2150</td>\n",
       "      <td>839</td>\n",
       "      <td>28.07 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>82.85 %</td>\n",
       "      <td>11.83 %</td>\n",
       "      <td>57.19 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                10386           6253    62.42 %\n",
       "REAL                 2150            839    28.07 %\n",
       "Percentage        82.85 %        11.83 %    57.19 %"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temos um total de 10386 VERDADEIROS POSITIVOS. \n",
    "- Temos um total de 6253 FALSOS NEGATIVOS.\n",
    "- Temos um total de 2150 FALSOS POSITIVOS.\n",
    "- Temos um total de 839 VERDADEIROS NEGATIVOS.\n",
    "\n",
    "Com isso, temos as seguintes métricas:\n",
    "- Acurácia: 57.19% = (TP+TN)/Total\n",
    "\n",
    "- Precisão: 82.85% = TP/P_predicted\n",
    "- False Omission Rate (FOR): 11.83% = FN/N_predicted\n",
    "- Recall/Sensibilidade/Probabilidade de detecção: 62.42% = TP/P\n",
    "- Especificidade: 28.07% = TN/N\n",
    "- Probabilidade de Alarme Falso = 1 - Especificidade = 71.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora, treinamos o modelo e testamos novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 0...\n",
      "Iteration 20 Loss 0.44600\n",
      "Iteration 40 Loss 0.31668\n",
      "Iteration 60 Loss 0.32156\n",
      "Iteration 80 Loss 0.36242\n",
      "Iteration 100 Loss 0.33671\n",
      "Iteration 120 Loss 0.30961\n",
      "Iteration 140 Loss 0.40002\n",
      "Iteration 160 Loss 0.28032\n",
      "Iteration 180 Loss 0.29681\n",
      "Iteration 200 Loss 0.31612\n",
      "Iteration 220 Loss 0.29692\n",
      "Iteration 240 Loss 0.25674\n",
      "Iteration 260 Loss 0.34980\n",
      "Iteration 280 Loss 0.32549\n",
      "Iteration 300 Loss 0.30501\n",
      "Iteration 320 Loss 0.25702\n",
      "Iteration 340 Loss 0.33320\n",
      "Iteration 360 Loss 0.37822\n",
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.27416\n",
      "Iteration 40 Loss 0.17406\n",
      "Iteration 60 Loss 0.36319\n",
      "Iteration 80 Loss 0.22675\n",
      "Iteration 100 Loss 0.30393\n",
      "Iteration 120 Loss 0.32933\n",
      "Iteration 140 Loss 0.22070\n",
      "Iteration 160 Loss 0.27466\n",
      "Iteration 180 Loss 0.30978\n",
      "Iteration 200 Loss 0.31370\n",
      "Iteration 220 Loss 0.21768\n",
      "Iteration 240 Loss 0.26396\n",
      "Iteration 260 Loss 0.23313\n",
      "Iteration 280 Loss 0.29257\n",
      "Iteration 300 Loss 0.30907\n",
      "Iteration 320 Loss 0.31404\n",
      "Iteration 340 Loss 0.38593\n",
      "Iteration 360 Loss 0.30588\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>16464</td>\n",
       "      <td>175</td>\n",
       "      <td>98.95 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>2079</td>\n",
       "      <td>910</td>\n",
       "      <td>30.44 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>88.79 %</td>\n",
       "      <td>83.87 %</td>\n",
       "      <td>88.52 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                16464            175    98.95 %\n",
       "REAL                 2079            910    30.44 %\n",
       "Percentage        88.79 %        83.87 %    88.52 %"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi obtida uma acurácia de 88.52% dessa vez, bem melhor! Resolvemos o problema do DeepFake então, certo? Bem... não.\n",
    "O dataset possui aproximadamente 40 mil imagens (93% do dataset) a mais de DeepFakes do que rostos verdadeiros.\n",
    "\n",
    "Aqui a nossa medida está sendo inteiramente a acurácia. Mas se observamos os resultados, vemos que dos 2989 rostos que não são DeepFakes, o modelo disse que 2079 deles são.\n",
    "O que está acontecendo aqui é que nosso dataset contém aproximadamente 40 mil rostos a mais de DeepFakes do que rostos reais, em uma proporção de aproximadamente 6:1. Dessa forma, o modelo encontrou um jeito fácil de ter uma alta acurácia sem necessariamente aprender a diferenciar um DeepFake de um rosto real: basta chutar que a maioria dos rostos são DeepFakes!\n",
    "Com essa tática, o modelo classificou corretamente 98.95% dos DeepFakes, mas se observarmos os rostos verdadeiros, de um total de 2989 rostos, o modelo classificou 2079 como DeepFakes, acertando apenas 30.44% dos rostos reais. Se você acreditar no modelo sempre que ele falar que determinada imagem é um DeepFake, você praticamente nunca deixará um DeepFake passar sem ser detectado. Excelente!... a não ser pelo fato de que ser tão diligente assim significa que você não está realmente poupando algum trabalho por usar uma rede neural para realizar esse serviço, uma vez que os vídeos que são de rostos reais também serão classificados como DeepFakes e será necessário inspecioná-los de qualquer jeito, caso contrário as consequências podem ser graves.\n",
    "\n",
    "\n",
    "Nosso modelo tem uma alta taxa de Falsos Positivos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que podemos fazer para tentar ajustar isso? No critério de avaliação, no caso, a nossa função de custo CrossEntropyLoss, existe a possibilidade de ponderar o peso que cada classe tem caso seja julgado corretamente ou incorretamente. Para isso, passamos uma lista com o peso de cada classe para a função de custo. Recarregamos o modelo e treinamos novamente com as devidas alterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.66378\n",
      "Iteration 40 Loss 0.57324\n",
      "Iteration 60 Loss 0.51859\n",
      "Iteration 80 Loss 0.56014\n",
      "Iteration 100 Loss 0.52884\n",
      "Iteration 120 Loss 0.60112\n",
      "Iteration 140 Loss 0.53312\n",
      "Iteration 160 Loss 0.54665\n",
      "Iteration 180 Loss 0.50956\n",
      "Iteration 200 Loss 0.67105\n",
      "Iteration 220 Loss 0.47249\n",
      "Iteration 240 Loss 0.43156\n",
      "Iteration 260 Loss 0.35011\n",
      "Iteration 280 Loss 0.42339\n",
      "Iteration 300 Loss 0.49906\n",
      "Iteration 320 Loss 0.37456\n",
      "Iteration 340 Loss 0.51171\n",
      "Iteration 360 Loss 0.39230\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.32898\n",
      "Iteration 40 Loss 0.48226\n",
      "Iteration 60 Loss 0.36133\n",
      "Iteration 80 Loss 0.38823\n",
      "Iteration 100 Loss 0.39688\n",
      "Iteration 120 Loss 0.46556\n",
      "Iteration 140 Loss 0.47131\n",
      "Iteration 160 Loss 0.38778\n",
      "Iteration 180 Loss 0.54917\n",
      "Iteration 200 Loss 0.39032\n",
      "Iteration 220 Loss 0.50269\n",
      "Iteration 240 Loss 0.47832\n",
      "Iteration 260 Loss 0.39394\n",
      "Iteration 280 Loss 0.56876\n",
      "Iteration 300 Loss 0.31022\n",
      "Iteration 320 Loss 0.46411\n",
      "Iteration 340 Loss 0.45022\n",
      "Iteration 360 Loss 0.41879\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 3x maior para a classe de não deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 3.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>14297</td>\n",
       "      <td>2342</td>\n",
       "      <td>85.92 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>991</td>\n",
       "      <td>1998</td>\n",
       "      <td>66.85 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>93.52 %</td>\n",
       "      <td>46.04 %</td>\n",
       "      <td>83.02 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                14297           2342    85.92 %\n",
       "REAL                  991           1998    66.85 %\n",
       "Percentage        93.52 %        46.04 %    83.02 %"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos melhorar sutilmente nosso resultado para a classe REAL. Podemos explorar um peso ainda maior agora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.66142\n",
      "Iteration 40 Loss 0.63493\n",
      "Iteration 60 Loss 0.57894\n",
      "Iteration 80 Loss 0.62384\n",
      "Iteration 100 Loss 0.53665\n",
      "Iteration 120 Loss 0.54088\n",
      "Iteration 140 Loss 0.53001\n",
      "Iteration 160 Loss 0.47315\n",
      "Iteration 180 Loss 0.56780\n",
      "Iteration 200 Loss 0.56354\n",
      "Iteration 220 Loss 0.51406\n",
      "Iteration 240 Loss 0.49521\n",
      "Iteration 260 Loss 0.54355\n",
      "Iteration 280 Loss 0.54263\n",
      "Iteration 300 Loss 0.48048\n",
      "Iteration 320 Loss 0.58414\n",
      "Iteration 340 Loss 0.52512\n",
      "Iteration 360 Loss 0.53464\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.49739\n",
      "Iteration 40 Loss 0.42896\n",
      "Iteration 60 Loss 0.54676\n",
      "Iteration 80 Loss 0.39877\n",
      "Iteration 100 Loss 0.54500\n",
      "Iteration 120 Loss 0.49910\n",
      "Iteration 140 Loss 0.53410\n",
      "Iteration 160 Loss 0.43038\n",
      "Iteration 180 Loss 0.43762\n",
      "Iteration 200 Loss 0.42918\n",
      "Iteration 220 Loss 0.46585\n",
      "Iteration 240 Loss 0.37796\n",
      "Iteration 260 Loss 0.48875\n",
      "Iteration 280 Loss 0.47058\n",
      "Iteration 300 Loss 0.53969\n",
      "Iteration 320 Loss 0.53361\n",
      "Iteration 340 Loss 0.49276\n",
      "Iteration 360 Loss 0.45491\n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet18.fc.in_features\n",
    "\n",
    "resnet18.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 8x maior para a classe de não deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 8.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.fc.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(n_epochs=2, model=resnet18, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>12105</td>\n",
       "      <td>4534</td>\n",
       "      <td>72.75 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>571</td>\n",
       "      <td>2418</td>\n",
       "      <td>80.9 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>95.5 %</td>\n",
       "      <td>34.78 %</td>\n",
       "      <td>73.99 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                12105           4534    72.75 %\n",
       "REAL                  571           2418     80.9 %\n",
       "Percentage         95.5 %        34.78 %    73.99 %"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet18)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem melhor! Infelizmente nossa acurácia geral caiu, mas o modelo conseguiu predizer mais rostos reais como reais e mais rostos falsos como falsos. Ainda há um longo caminho a se percorrer. Por hora, vamos tentar diminuir um pouco o learning rate e aumentar o número de épocas utilizando um modelo mais poderoso e observar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining epoch 1...\n",
      "Iteration 20 Loss 0.74290\n",
      "Iteration 40 Loss 0.69050\n",
      "Iteration 60 Loss 0.64445\n",
      "Iteration 80 Loss 0.63979\n",
      "Iteration 100 Loss 0.59284\n",
      "Iteration 120 Loss 0.59329\n",
      "Iteration 140 Loss 0.58190\n",
      "Iteration 160 Loss 0.55730\n",
      "Iteration 180 Loss 0.57475\n",
      "Iteration 200 Loss 0.58001\n",
      "Iteration 220 Loss 0.53973\n",
      "Iteration 240 Loss 0.56590\n",
      "Iteration 260 Loss 0.55069\n",
      "Iteration 280 Loss 0.53715\n",
      "Iteration 300 Loss 0.53509\n",
      "Iteration 320 Loss 0.53396\n",
      "Iteration 340 Loss 0.54495\n",
      "Iteration 360 Loss 0.54519\n",
      "Beggining epoch 2...\n",
      "Iteration 20 Loss 0.51861\n",
      "Iteration 40 Loss 0.54998\n",
      "Iteration 60 Loss 0.50373\n",
      "Iteration 80 Loss 0.51190\n",
      "Iteration 100 Loss 0.50706\n",
      "Iteration 120 Loss 0.52625\n",
      "Iteration 140 Loss 0.50254\n",
      "Iteration 160 Loss 0.52229\n",
      "Iteration 180 Loss 0.49917\n",
      "Iteration 200 Loss 0.51160\n",
      "Iteration 220 Loss 0.50501\n",
      "Iteration 240 Loss 0.50059\n",
      "Iteration 260 Loss 0.50135\n",
      "Iteration 280 Loss 0.46850\n",
      "Iteration 300 Loss 0.47726\n",
      "Iteration 320 Loss 0.51384\n",
      "Iteration 340 Loss 0.49972\n",
      "Iteration 360 Loss 0.51220\n",
      "Beggining epoch 3...\n",
      "Iteration 20 Loss 0.47358\n",
      "Iteration 40 Loss 0.48821\n",
      "Iteration 60 Loss 0.49637\n",
      "Iteration 80 Loss 0.49251\n",
      "Iteration 100 Loss 0.49014\n",
      "Iteration 120 Loss 0.49817\n",
      "Iteration 140 Loss 0.48201\n",
      "Iteration 160 Loss 0.47955\n",
      "Iteration 180 Loss 0.48280\n",
      "Iteration 200 Loss 0.46180\n",
      "Iteration 220 Loss 0.49370\n",
      "Iteration 240 Loss 0.47910\n",
      "Iteration 260 Loss 0.49834\n",
      "Iteration 280 Loss 0.47183\n",
      "Iteration 300 Loss 0.47228\n",
      "Iteration 320 Loss 0.51562\n",
      "Iteration 340 Loss 0.48200\n",
      "Iteration 360 Loss 0.46087\n"
     ]
    }
   ],
   "source": [
    "resnet34 = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "# Congela o treinamento para todas as camadas de \"features\"\n",
    "for param in resnet34.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = resnet34.fc.in_features\n",
    "\n",
    "resnet34.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Aqui estou experimentado colocar um peso 9x maior para a classe de não deepfake\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.FloatTensor([1.0, 7.0]))\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet34.fc.parameters(), lr=0.4e-3)\n",
    "\n",
    "training_loop(n_epochs=3, model=resnet34, loss_function=loss_function, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAKE_predicted</th>\n",
       "      <th>REAL_predicted</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>12394</td>\n",
       "      <td>4245</td>\n",
       "      <td>74.49 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REAL</th>\n",
       "      <td>604</td>\n",
       "      <td>2385</td>\n",
       "      <td>79.79 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>95.35 %</td>\n",
       "      <td>35.97 %</td>\n",
       "      <td>75.3 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FAKE_predicted REAL_predicted Percentage\n",
       "FAKE                12394           4245    74.49 %\n",
       "REAL                  604           2385    79.79 %\n",
       "Percentage        95.35 %        35.97 %     75.3 %"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred, all_labels = eval_loop(resnet34)\n",
    "c_mat = createDataFrame_conf_matrix(all_pred, all_labels)\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um resultado promissor. Embora nossa acurácia geral tenha aumentado apenas um pouco para 75.3%, fomos capazes de classificar corretamente 74.49% dos deepfakes e 79.79% dos reais. Estamos indo certo em alguma direção!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet34.state_dict(), './Saved Models/resnet34_pretrained_trainedover2folders.pt')\n",
    "\n",
    "#torch.save({\n",
    "#            'epoch': epoch,\n",
    "#            'model_state_dict': model.state_dict(),\n",
    "#            'optimizer_state_dict': optimizer.state_dict(),\n",
    "#            'loss': loss,\n",
    "#            ...\n",
    "#            }, PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
