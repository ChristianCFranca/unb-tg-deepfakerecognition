{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando modelo do Fastai Pré Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastai_to_pytorch(base_model, path, path_to_model):\n",
    "    all_data = ImageDataLoaders.from_folder(path=path, # Indicamos aqui o caminho que contém as 2 pastas FAKE e REAL\n",
    "                                        train='train', # Indicamos aqui o nome da pasta que é de treinamento. Observe que como nosso dataset não possui ainda a divisão, deixamos padrão\n",
    "                                        valid='valid', # O mesmo vale para a parte de validação\n",
    "                                        seed=42, # Fornecemos um seed para que a divisão seja sempre a mesma, não importa a execução\n",
    "                                        bs=1, # Fornecemos o tamanho do lote de imagens\n",
    "                                        item_tfms=Resize(224), # Indicamos que queremos redimensionar os itens individuais para o tamanho indicado (224x224 no caso)\n",
    "                                        #batch_tfms=Normalize.from_stats(*imagenet_stats), # Indicamos que queremos normalizar as imagens com os mesmos status de quando foram treinadas \n",
    "                                        num_workers=0 # Esse argumento é necessário para funcionar no windows\n",
    "                                       )\n",
    "    learn = cnn_learner(dls=all_data, \n",
    "                        arch=base_model)\n",
    "\n",
    "    learn.load(\"../../\" + path_to_model)\n",
    "    \n",
    "    return learn.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = fastai_to_pytorch(resnet18, path=\"./Dummy Dataset\", path_to_model=\"models/resnet18_best_mcc_final\")\n",
    "model_2 = fastai_to_pytorch(resnet18, path=\"./Dummy Dataset\", path_to_model=\"models/resnet18_best_mcc_F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, real_groups, \n",
    "                 fake_groups, \n",
    "                 path=\"./Faces Dataset Transformer\", \n",
    "                 path_to_model=\"models/resnet18_best_mcc_final\",\n",
    "                 device=\"cuda\"):\n",
    "        \n",
    "        self.device = device\n",
    "        self.path = path\n",
    "        self.groups = real_groups + fake_groups\n",
    "        self.labels = torch.cat((torch.zeros(len(real_groups)).long(),\n",
    "                                 torch.ones(len(fake_groups)).long()\n",
    "                                ), axis=0)\n",
    "        self.model = self.fastai_to_pytorch(base_model=resnet18, path=path, path_to_model=path_to_model).eval().to(device)\n",
    "        self.model[1][8] = nn.Identity()\n",
    "        self.size = 224\n",
    "        self.retrieve_tensors()\n",
    "        \n",
    "    def fastai_to_pytorch(self, base_model, path, path_to_model):\n",
    "        all_data = ImageDataLoaders.from_folder(path=path,\n",
    "                                        train='train',\n",
    "                                        valid='valid',\n",
    "                                        valid_pct=0.1,\n",
    "                                        seed=42,\n",
    "                                        bs=1,\n",
    "                                        item_tfms=Resize(224),\n",
    "                                        num_workers=0)\n",
    "        \n",
    "        learn = cnn_learner(dls=all_data, \n",
    "                            arch=base_model)\n",
    "\n",
    "        learn.load(\"../../\" + path_to_model)\n",
    "\n",
    "        return learn.model\n",
    "        \n",
    "    def retrieve_tensors(self):\n",
    "        self.data = torch.tensor([]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for groups in self.groups:\n",
    "                out = torch.tensor([]).to(self.device)\n",
    "                img_zeros = torch.tensor([]).to(self.device)\n",
    "                for i, image_path in enumerate(groups):\n",
    "                    if image_path != \"__PAD__\":\n",
    "                        out = torch.cat((\n",
    "                            (Normalize(*imagenet_stats)(to_tensor(Image.open(image_path).resize((self.size, self.size))))).unsqueeze(0).to(self.device),\n",
    "                            out), axis=0)\n",
    "                    else:\n",
    "                        img_zeros = torch.zeros(len(self.groups[i])-i, 512).to(self.device)\n",
    "                        break\n",
    "                \n",
    "                out = torch.cat((self.model(out), img_zeros), axis=0)\n",
    "                \n",
    "                self.data = torch.cat((out.unsqueeze(0), self.data), axis=0)\n",
    "                \n",
    "        self.data = self.data.detach().cpu()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.data[i], self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, batch_size=32, path=\"./Faces Dataset Transformer\", path_to_model=\"models/resnet18_best_mcc_final\"):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage):\n",
    "        \n",
    "        base_dir = Path(self.path)\n",
    "        all_real_paths = list((base_dir/\"REAL\").glob(\"*jpg\"))\n",
    "        all_fake_paths = list((base_dir/\"FAKE\").glob(\"*jpg\"))\n",
    "        \n",
    "        groups = [[], []]\n",
    "        max_len = 0\n",
    "        init_name = None\n",
    "        for j, groups_path in enumerate([all_real_paths, all_fake_paths]):\n",
    "            for i, path in enumerate(groups_path):\n",
    "                path_stem = path.stem\n",
    "                folder, name, pos = path_stem.split(\" \")\n",
    "                if name != init_name:\n",
    "                    if i != 0:\n",
    "                        if len(group) > max_len:\n",
    "                            max_len = len(group)\n",
    "                        indexes = np.array([group_path.stem.split(\" \")[2].replace(\".jpg\",\"\") for group_path in group], dtype=np.int8)\n",
    "                        sort = indexes.argsort()\n",
    "                        groups[j].append([group[s] for s in sort])\n",
    "                    init_name = name\n",
    "                    group = []\n",
    "                else:\n",
    "                    group.append(path)\n",
    "\n",
    "        for group in groups:\n",
    "            for i, gr in enumerate(group):\n",
    "                n_append = max_len - len(gr)\n",
    "                if n_append != 0:\n",
    "                    group[i] = group[i] + [\"__PAD__\" for n in range(n_append)]\n",
    "        \n",
    "        self.real_groups, self.fake_groups = groups[0], groups[1]\n",
    "        \n",
    "        self.dfdataset = DFDataset(self.real_groups, self.fake_groups)\n",
    "        ratio = 0.7\n",
    "        self.dfdataset_train, self.dfdataset_val = torch.utils.data.random_split(self.dfdataset, [int(ratio*len(self.dfdataset)), \n",
    "                                                       len(self.dfdataset) - int(ratio*len(self.dfdataset))])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_dl = torch.utils.data.DataLoader(self.dfdataset_train, batch_size=self.batch_size, shuffle=True)\n",
    "        return train_dl\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        val_dl = torch.utils.data.DataLoader(self.dfdataset_val, batch_size=self.batch_size, shuffle=True)\n",
    "        return val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CNNFeatureTransformerWithoutModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(CNNFeatureTransformerWithoutModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=2048), num_layers=6)\n",
    "        self.decoder = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, 0, :] # Classificação\n",
    "    \n",
    "class CNNFeatureTransformerWithModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, model, max_len):\n",
    "        super(CNNFeatureTransformerWithModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=2048), num_layers=6)\n",
    "        self.decoder = nn.Linear(d_model, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        num_samples = x.shape[1]\n",
    "        \n",
    "        x = x.reshape(x.shape[0]*x.shape[1], x.shape[2], x.shape[3], x.shape[4])\n",
    "        x = self.model(x).reshape(batch_size, num_samples, -1)\n",
    "         \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, 0, :] # Classificação\n",
    "    \n",
    "cnnft_modelWM = CNNFeatureTransformerWithModel(d_model=512, model=model, max_len=2000)\n",
    "cnnft_model = CNNFeatureTransformerWithoutModel(d_model=512, max_len=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvFusionTransformer(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=2048), num_layers=6)\n",
    "        self.decoder = nn.Linear(d_model, 2)\n",
    "        self.accuracy = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x): # No lightning são as ações de inferência\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, 0, :]\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder(x)[:, 0, :]\n",
    "        loss = F.cross_entropy(x, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.decoder(x)[:, 0, :]\n",
    "        loss = F.cross_entropy(x, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.accuracy(F.softmax(x, dim=1), y)\n",
    "        self.log('valid_acc', self.accuracy, on_step=True, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos constatar a eficácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "current_model = ConvFusionTransformer(d_model=512, max_len=200)\n",
    "dfmodel = DFDataModule()\n",
    "trainer = pl.Trainer(gpus='0', callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | pos_encoder | PositionalEncoding | 0     \n",
      "1 | transformer | TransformerEncoder | 18.9 M\n",
      "2 | decoder     | Linear             | 1.0 K \n",
      "3 | accuracy    | Accuracy           | 0     \n",
      "---------------------------------------------------\n",
      "18.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 M    Total params\n",
      "75.661    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b617246ce95b4e6fa5df4a6b3389f285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(current_model, datamodule=dfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(cnnft_model, wm):\n",
    "    cnnft_model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        if wm:\n",
    "            for features, targets in test_dlWM:\n",
    "                features, targets = features.to(device), targets\n",
    "                out = cnnft_model(features)\n",
    "\n",
    "                predictions += out.max(dim=1)[1].tolist()\n",
    "\n",
    "                labels += targets.tolist()\n",
    "        else:\n",
    "            for features, targets in test_dl:\n",
    "                features, targets = features.to(device), targets\n",
    "                out = cnnft_modelWM(features)\n",
    "\n",
    "                predictions += out.max(dim=1)[1].tolist()\n",
    "\n",
    "                labels += targets.tolist()\n",
    "            \n",
    "    right = np.sum(np.array(labels) == np.array(predictions))\n",
    "    total = len(labels)\n",
    "    acc_for_zero = []\n",
    "    acc_for_one = []\n",
    "    for i, j in zip(labels, predictions):\n",
    "        if i == 1 and i == j:\n",
    "            acc_for_one.append(True)\n",
    "        elif i == 1 and i != j:\n",
    "            acc_for_one.append(False)\n",
    "        elif i == 0 and i == j:\n",
    "            acc_for_zero.append(True)\n",
    "        else:\n",
    "            acc_for_zero.append(False)\n",
    "    \n",
    "    acc_for_zero = np.array(acc_for_zero)\n",
    "    acc_for_one = np.array(acc_for_one)\n",
    "    print(f\"Accuracy: {right/total*100: .2f}%\\t| Accuracy for FAKE: {acc_for_zero.sum()/len(acc_for_zero)*100:.2f}%\\t| Accuracy for REAL: {acc_for_one.sum()/len(acc_for_one)*100:.2f}%\")\n",
    "    return np.array(labels), np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  68.99%\t| Accuracy for FAKE: 56.58%\t| Accuracy for REAL: 80.49%\n"
     ]
    }
   ],
   "source": [
    "labels2, preds2 = get_accuracy(cnnft_modelWM, wm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
