{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desperate-pulse",
   "metadata": {},
   "source": [
    "# Verificação do tempo de inferência para uma solução end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-ivory",
   "metadata": {},
   "source": [
    "As dependências necessárias são importadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bright-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Para adquirir o modelo pré-treinado\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Para iterar pelos vídeos e tornar interativo se desejado\n",
    "import cv2\n",
    "\n",
    "# Para extrair os rostos dos frames\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# Útil para realizar operações em vetores\n",
    "import numpy as np\n",
    "\n",
    "# Dicionario para contabilizar os FAKE e os REAL\n",
    "from collections import defaultdict\n",
    "\n",
    "# Limpa prints extras nas células\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-pollution",
   "metadata": {},
   "source": [
    "Define-se o dispositivo que irá rodar as computações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "italian-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos um device onde os tensores estarão sendo processados\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rental-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações para a MTCNN\n",
    "IMAGE_SIZE = 224\n",
    "MARGIN = 0\n",
    "MIN_FACE_SIZE = 90\n",
    "THRESHOLDS = [0.68, 0.75, 0.80]\n",
    "POST_PROCESS = False\n",
    "SELECT_LARGEST = True\n",
    "KEEP_ALL = False\n",
    "DEVICE = device\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "mtcnn = MTCNN(image_size=IMAGE_SIZE,\n",
    "              margin=MARGIN, \n",
    "              min_face_size=MIN_FACE_SIZE, \n",
    "              thresholds=THRESHOLDS,\n",
    "              post_process=POST_PROCESS,\n",
    "              select_largest=SELECT_LARGEST, \n",
    "              keep_all=KEEP_ALL, \n",
    "              device=device)\n",
    "\n",
    "path_to_learner = Path('./models/final_learner.pkl')\n",
    "learner = load_learner(path_to_learner, cpu=device.type == 'cpu') # As inferências nas imagens serão feitas pela CPU uma vez que será feito vídeo por vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-annual",
   "metadata": {},
   "source": [
    "Define-se uma função que extrai os rostos dos vídeos utilizando a MTCNN e os guarda em lista lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "informative-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_from_video(video_path, padding=0, resize_factor=0.6, check_every_frame=30, show_frames=False):\n",
    "    \n",
    "    # Captura o vídeo no path\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "    except:\n",
    "        print(\"Ocorreu um erro ao carregar o vídeo. Certifique-se de que é um arquivo de vídeo válido.\")\n",
    "        return []\n",
    "    \n",
    "    # Pega, em inteiros, a quantidade de frames do vídeo\n",
    "    v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    faces = []\n",
    "    for i in range(1, v_len + 1):\n",
    "        success = cap.grab()\n",
    "        # Apertar a tecla 'q' para sair do vídeo.\n",
    "        if not success:\n",
    "            continue\n",
    "        if  i % check_every_frame == 0:\n",
    "            success, frame = cap.retrieve()\n",
    "            if not success:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if success: # Sucesso na leitura\n",
    "            # Obtém o frame como PIL Image (ele é capturado no formato BGR porém a MTCNN espera no formato RGB)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = Image.fromarray(frame_rgb)\n",
    "            if show_frames:\n",
    "                frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "                cv2.imshow('frame', frame)\n",
    "\n",
    "            boxes, _ = mtcnn.detect(frame_rgb) # Detecta as imagens. O método detect só aceita numpy arrays\n",
    "            \n",
    "            if boxes is not None: # Só entra se rostos forem detectados\n",
    "                for box in boxes: # Para cada uma das bouding boxes encontradas em um único frame (a princípio só deve ter uma)\n",
    "                    box = [int(b) for b in box]\n",
    "                    # Extrai a face\n",
    "                    face = frame_rgb.crop(box=(box[0]-padding, \n",
    "                                               box[1]-padding, \n",
    "                                               box[2]+padding, \n",
    "                                               box[3]+padding))\n",
    "                    faces.append(PILImage(face))\n",
    "\n",
    "                    if show_frames:\n",
    "                        frame = cv2.resize(frame, (int(frame.shape[1]*resize_factor), int(frame.shape[0]*resize_factor)))\n",
    "                        cv2.imshow('frame', frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    \n",
    "                \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assumed-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(learner, faces, verbose=False):\n",
    "    predicts = []\n",
    "    predicts_dict = defaultdict(lambda: 0)\n",
    "    for i, face in enumerate(faces):\n",
    "        res = learner.predict(face)\n",
    "        clear_output()\n",
    "        if verbose:\n",
    "            print(f\"Predição realizada para a face {i+1}\")\n",
    "        predicts.append(res[1].item())\n",
    "        predicts_dict[res[0]] += 1\n",
    "    print('-'*100)\n",
    "    print(F\"Resultados individuais: Quantidade de \\033[91m FAKES \\033[0m: {predicts_dict['FAKE']} | Quantidade de \\033[92m REALS \\033[0m: {predicts_dict['REAL']}\")\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supreme-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_prediction_from_predictions(predictions, roh=2.75):\n",
    "    print(\"-\"*100)\n",
    "    print(f\"Utilizando regra com ρ = {roh}...\\n\")\n",
    "    if not isinstance(predictions, np.ndarray):\n",
    "        predictions = np.array(predictions)\n",
    "    qtd_fakes = np.count_nonzero(predictions == 0)\n",
    "    qtd_reals = np.count_nonzero(predictions == 1)\n",
    "    \n",
    "    return 'FAKE' if qtd_fakes >= roh*qtd_reals else 'REAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sublime-sullivan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('Kaggle Dataset/dfdc_train_part_0/aaqaifqrwn.mp4'),\n",
       " Path('Kaggle Dataset/dfdc_train_part_0/aayrffkzxn.mp4'),\n",
       " Path('Kaggle Dataset/dfdc_train_part_0/abhggqdift.mp4'),\n",
       " Path('Kaggle Dataset/dfdc_train_part_0/acagallncj.mp4'),\n",
       " Path('Kaggle Dataset/dfdc_train_part_0/acdkfksyev.mp4')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"dfdc_train_part_0\"\n",
    "list_of_paths = list(Path(f\"./Kaggle Dataset/{folder}\").glob(\"*.mp4\"))[:10] # 10 primeiros vídeos\n",
    "list_of_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "likely-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Resultados individuais: Quantidade de \u001b[91m FAKES \u001b[0m: 10 | Quantidade de \u001b[92m REALS \u001b[0m: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Utilizando regra com ρ = 2.75...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_every_frame=30\n",
    "rho=2.75\n",
    "\n",
    "tempo_corrido = []\n",
    "qtde_rostos = []\n",
    "for path in list_of_paths:\n",
    "    inicio = time.time()\n",
    "    \n",
    "    faces = extract_faces_from_video(path, check_every_frame=check_every_frame)\n",
    "    if len(faces) == 0:\n",
    "        print(\"Não foi possível detectar faces humanas no vídeo fornecido.\")\n",
    "    else:\n",
    "        preds = get_predictions(learner, faces)\n",
    "        final_res = get_final_prediction_from_predictions(preds, rho)\n",
    "    \n",
    "    final = time.time()\n",
    "\n",
    "    tempo_corrido.append((final-inicio)) # Pegamos o tempo total e dividimos pela quantidade de faces\n",
    "    qtde_rostos.append(len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "boxed-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_corrido = np.array(tempo_corrido)\n",
    "qtde_rostos = np.array(qtde_rostos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adequate-apartment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.82597709, 2.96306109, 2.85306478, 2.73500085, 2.84399891,\n",
       "        2.9134295 , 2.87600112, 3.06052232, 2.88699961, 2.77999902]),\n",
       " array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10]),\n",
       " array([282.5977087 , 296.30610943, 285.30647755, 273.50008488,\n",
       "        284.3998909 , 291.34294987, 287.60011196, 306.05223179,\n",
       "        288.69996071, 277.99990177]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempo_corrido, qtde_rostos, tempo_corrido*1000/qtde_rostos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "typical-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8738054275512694, 0.08732196321761623)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempo_corrido.mean(), tempo_corrido.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
